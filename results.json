[
  {
    "question": "What are the main objectives of the study on Table Information Seeking (TIS) in Large Language Models (LLMs)?",
    "answer": "The main objectives of the study on Table Information Seeking (TIS) in Large Language Models (LLMs) include investigating the effectiveness of various LLMs in interpreting tabular data using different prompting strategies and data formats. The study analyzes the performance of models on six benchmarks for table-related tasks such as question-answering and fact-checking. A key aspect of the research is the novel assessment of LLMs’ performance on image-based table representations, in addition to text-based representations. The study aims to provide insights into optimizing LLMs for table-related tasks, including examining the effectiveness of different representations and prompting methods on LLM performance in processing tabular data [1].",
    "references": [
      "arXiv:2402.12424v5 [cs.LG] 17 Oct 2024\nTables as Texts or Images:\nEvaluating the Table Reasoning Ability of LLMs and MLLMs\nNaihao Deng* , Zhenjie Sun* , Ruiqi He , Aman Sikka ,\nYulong Chen , Lin Ma , Yue Zhang , Rada Mihalcea\nUniversity of Michigan University of Cambridge Westlake University\n{dnaihao, zjsun}@umich.edu\nAbstract\nIn this paper, we investigate the effectiveness\nof various LLMs in interpreting tabular data\nthrough different prompting strategies and data\nformats. Our analyses extend across six bench-\nmarks for table-related tasks such as question-\nanswering and fact-checking. We introduce\nfor the first time the assessment of LLMs’\nperformance on image-based table represen-\ntations. Specifically, we compare five text-\nbased and three image-based table represen-\ntations, demonstrating the role of representa-\ntion and prompting on LLM performance. Our\nstudy provides insights into the effective use\nof LLMs on table-related tasks. Our data is\navailable at: https://github.com/dnaihao/\nTables-as-Texts-or-Images.\n1 Introduction\nRecent years have witnessed an explosion of Large\nLanguage Models (LLMs), with impressive per-\nformance on various Natural Language Processing\n(NLP) tasks (Brown et al., 2020; Touvron et al.,\n2023; Team et al., 2023). Research to date has\nexamined the performance of LLMs for various\naspects and abilities (Bang et al., 2023b; Bubeck\net al., 2023; Akter et al., 2023), but their effec-\ntiveness on structured data such as tables is less\nexplored.\nUnlike unstructured text, tables are systemat-\nically organized structures of a large amount of\ninformation. This characteristic makes tabular\ndata serve as the foundations for numerous ap-\nplications, including medical diagnostics, virtual\npersonal assistants, customer relationship manage-\nment (Hemphill et al., 1990; Dahl et al., 1994;\nAkhtar et al., 2022; Xie et al., 2022), etc.\nThe evaluation of LLMs on processing tabular\ndata involves many challenges. First, there are\nmany ways to represent the information in tables.\n*Contributed equally to this work. Appendix A lists the\ndetailed contributions.\nIf we represent the table in pure text, we may use\nnaive linearization or insert brackets to better rep-\nresent table structures. Meanwhile, emerging mul-\ntimodal LLMs like GPT-4 (Achiam et al., 2023)\nand Gemini (Team et al., 2023) offer image-based\napproaches, where we can pass the table as im-\nages to the LLMs. In such cases, visual cues like\ncolor highlighting in tables can influence outcomes.\nSecond, diverse prompting methods for text may\nalso apply to tabular data, which can yield varied\nresults (Wei et al., 2022). Furthermore, the tasks\ninvolving tabular data are diverse, including table\nfact-checking (Chen et al., 2019) and table question\nanswering (Pasupat and Liang, 2015), and table-to-\ntext generation (Novikova et al., 2017), etc.\nIn this paper, we systematically evaluate model\nperformance on tabular data for both textual LLMs\nand multi-modal LLMs. Specifically, we investi-\ngate several research questions, including the ef-\nfectiveness of image-based representation of tab-\nular data and how different text-based or image-\nbased prompt methods affect LLMs’ performance\non table-related tasks. In addition, we provide anal-\nysis and hypothesis of LLMs’ behaviors. Our find-\nings include:\n• LLMs maintain decent performance when we\nuse image-based table representations. Some-\ntimes, image-based table representations can\nmake LLMs perform better.\n• There are nuances in the prompting design for\ntable-related tasks, revealed by our comparisons\nof various prompting methods for text- and\nimage-based table representations.\nTo the best of our knowledge, we are the first to\nstudy how LLMs perform with image-based table\nrepresentations. We believe this paper draws new\ninsights into optimizing table-based information\nprocessing.\nText-Based\nImage-Based\nTask Specific Prompt\nOutput\nTableQA\nTableQA\nVanilla-T\nRow-Identifier\nBracket\nColumn-JSON\nRow-JSON",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8"
    ]
  },
  {
    "question": "How does the newly introduced benchmark, TabIS, differ from previous evaluation methods for table information extraction?, ",
    "answer": "The newly introduced benchmark, TabIS, differs from previous evaluation methods for table information extraction primarily by focusing on the combined evaluation of both text-based and image-based table representations. Unlike earlier methods that may have assessed either text or images in isolation, TabIS allows for the exploration of how these different representations can be utilized concurrently to assess their impact on the performance of LLMs. This benchmark aims to comprehensively evaluate the ability of large language models to process and understand tabular data in various formats, thereby providing a more holistic view of their capabilities in table information extraction tasks [2].",
    "references": [
      "53.0 54.1 50.0\n46.0 46.3\n47.0 52.1\n41.3\n58.0\n65.0\n36.0 44.5\n45.5\n44.6 43.5\n38.0\n38.3 43.4\n21.1\nGMNpro(T)\nGMNpro(V)\nGPT-4 (T)\nGPT-4 (V)\nFinQA LogicNLG TabFact WikiTQ E2E Totto\nFigure 9: Performance comparison between passing the text versus image representations of tables to GPT-4 and\nGeminiPro across FinQA, LogicNLG, TabFact, and WikiTQ by accuracy, and E2E and ToTTo by ROUGE-L scores.\nWe use the linearized table (Vanilla-T) as the text-based representation, the original table image (Vanilla-V) as the\nimage-based representation, and CoT prompting.\n80\n60\n40\n20\n0\n64.0\n56.0\n72.0 77.0\n58.2 63.0\n29.0 33.0 52.0 54.1\n53.0\n30.1\n31.7\n45.3\n44.9 42.8\n42.4\n45.0\n42.3\n39.2\n35.9\n35.6\n35.0 28.0\nGMNpro(T)\nGMNpro(V)\nGPT-4 (T)\nGPT-4 (V)\nFinQA LogicNLG TabFact WikiTQ E2E Totto\nFigure 10: Performance comparison between passing the text versus image representations of tables to GPT-4 and\nGeminiPro across FinQA, LogicNLG, TabFact, and WikiTQ by accuracy, and E2E and ToTTo by ROUGE-L scores.\nWe use the linearized table (Vanilla-T) as the text-based representation, the original table image (Vanilla-V) as the\nimage-based representation, and expert prompting.\nGPT-4 manages to reconstruct a 4-row×10-column\ntable with over 100 words (Table 16).\nBut GPT-4 is more prone to hallucination or\nmessing up with the spatial relations when the table\ngets more complicated. For instance, for Table 17,\nGPT-4 reconstructs it as Table 18. As expected,\nGPT-4 fails to answer the corresponding question\nto this table when using a table image as the in-\nput. However, we notice that when using the text\nrepresentation of the table, GPT-4 also fails to an-\nswer this question. This aligns with what we have\nreported in.\nAs expected, GPT-4 fails to answer the corre-\nsponding question to this table when using a table\nimage as the input. However, we notice that when\nusing the text representation of the table, GPT-4\nalso fails to answer this question. This aligns with\nwhat we have reported in Figure 3 that On FinQA,\nGPT-4 better leverages the image representations\nthan the text representations in general.\nC.5 RQ6: Do multimodal LLMs benefit from\nhaving both formats simultaneously as\ninput?\nTest: the six datasets.\nWe test GPT-4 with vanilla prompting for\nTL;DR Answer: Not generally true.\nFull Answer: As shown in Table 19, it is not\ngenerally true that multimodal LLMs benefit from\nhaving both formats simultaneously as input.\nThere are cases where passing both image and\ntext representations would increase the perfor-\nmance (e.g. GPT-4 on WikiTQ and FinQA). In\nother cases, the performance is comparable to ei-\nther passing tables as text or image representation\nor lies in between.\nD Comparison of Llama Models and\nGPT-4 Models\nTable 20 provides the details of what combina-\ntion of table representation and prompting method\nyields the best performance with respect to the\nLlama-70B and GPT-4 models.\nE Prompt Examples\nFigure 11 gives an example of how we construct\nour prompt for an instance in WikiTQ.\nF LLMs’ Generation Examples on Each\nDataset\nFigure 12 gives examples for WikiTQA, TabFact,\nLogicNLG, and FinQA datasets we use, how many\ncombinations of LLMs, table representations, and\nprompting techniques can answer the question cor-\nDistrict Location Communities served\nAgape Christian Academy Burton Township, Ohio and Troy Township, Ohio Accepts applications prior to the start of each school year\n…\nQuestion: where is saint anslem school located?\nDistrict Location Communities served",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8"
    ]
  },
  {
    "question": "What were the performance results of various LLMs tested using the TabIS benchmark, particularly regarding their understanding of table structures?\n",
    "answer": "The performance results of various LLMs tested using the TabIS benchmark indicate that models like GPT-4 and Geminipro manage to achieve decent performance levels when understanding table structures. For instance, GPT-4 shows accuracy rates of 52.5% when using vanilla prompting with text-based representations, and 60.3% when utilizing the chain-of-thought (CoT) prompting method with linearized table representation. Similarly, the effectiveness of their understanding is demonstrated through the performance variations observed across different prompting methods and representations, underscoring their strong abilities in table comprehension tasks [1]. \n\nAdditional findings highlight that chain-of-thought prompting generally assists LLMs in reasoning over image-based tables, with GPT-4 achieving an accuracy improvement from 51.8% with vanilla prompting to 60.2% with CoT prompting when colors were used to distinguish rows in images [2]. This suggests that effective prompting strategies can significantly enhance the models' understanding of table layouts and the relationships between data points.",
    "references": [
      "arXiv:2402.12424v5 [cs.LG] 17 Oct 2024\nTables as Texts or Images:\nEvaluating the Table Reasoning Ability of LLMs and MLLMs\nNaihao Deng* , Zhenjie Sun* , Ruiqi He , Aman Sikka ,\nYulong Chen , Lin Ma , Yue Zhang , Rada Mihalcea\nUniversity of Michigan University of Cambridge Westlake University\n{dnaihao, zjsun}@umich.edu\nAbstract\nIn this paper, we investigate the effectiveness\nof various LLMs in interpreting tabular data\nthrough different prompting strategies and data\nformats. Our analyses extend across six bench-\nmarks for table-related tasks such as question-\nanswering and fact-checking. We introduce\nfor the first time the assessment of LLMs’\nperformance on image-based table represen-\ntations. Specifically, we compare five text-\nbased and three image-based table represen-\ntations, demonstrating the role of representa-\ntion and prompting on LLM performance. Our\nstudy provides insights into the effective use\nof LLMs on table-related tasks. Our data is\navailable at: https://github.com/dnaihao/\nTables-as-Texts-or-Images.\n1 Introduction\nRecent years have witnessed an explosion of Large\nLanguage Models (LLMs), with impressive per-\nformance on various Natural Language Processing\n(NLP) tasks (Brown et al., 2020; Touvron et al.,\n2023; Team et al., 2023). Research to date has\nexamined the performance of LLMs for various\naspects and abilities (Bang et al., 2023b; Bubeck\net al., 2023; Akter et al., 2023), but their effec-\ntiveness on structured data such as tables is less\nexplored.\nUnlike unstructured text, tables are systemat-\nically organized structures of a large amount of\ninformation. This characteristic makes tabular\ndata serve as the foundations for numerous ap-\nplications, including medical diagnostics, virtual\npersonal assistants, customer relationship manage-\nment (Hemphill et al., 1990; Dahl et al., 1994;\nAkhtar et al., 2022; Xie et al., 2022), etc.\nThe evaluation of LLMs on processing tabular\ndata involves many challenges. First, there are\nmany ways to represent the information in tables.\n*Contributed equally to this work. Appendix A lists the\ndetailed contributions.\nIf we represent the table in pure text, we may use\nnaive linearization or insert brackets to better rep-\nresent table structures. Meanwhile, emerging mul-\ntimodal LLMs like GPT-4 (Achiam et al., 2023)\nand Gemini (Team et al., 2023) offer image-based\napproaches, where we can pass the table as im-\nages to the LLMs. In such cases, visual cues like\ncolor highlighting in tables can influence outcomes.\nSecond, diverse prompting methods for text may\nalso apply to tabular data, which can yield varied\nresults (Wei et al., 2022). Furthermore, the tasks\ninvolving tabular data are diverse, including table\nfact-checking (Chen et al., 2019) and table question\nanswering (Pasupat and Liang, 2015), and table-to-\ntext generation (Novikova et al., 2017), etc.\nIn this paper, we systematically evaluate model\nperformance on tabular data for both textual LLMs\nand multi-modal LLMs. Specifically, we investi-\ngate several research questions, including the ef-\nfectiveness of image-based representation of tab-\nular data and how different text-based or image-\nbased prompt methods affect LLMs’ performance\non table-related tasks. In addition, we provide anal-\nysis and hypothesis of LLMs’ behaviors. Our find-\nings include:\n• LLMs maintain decent performance when we\nuse image-based table representations. Some-\ntimes, image-based table representations can\nmake LLMs perform better.\n• There are nuances in the prompting design for\ntable-related tasks, revealed by our comparisons\nof various prompting methods for text- and\nimage-based table representations.\nTo the best of our knowledge, we are the first to\nstudy how LLMs perform with image-based table\nrepresentations. We believe this paper draws new\ninsights into optimizing table-based information\nprocessing.\nText-Based\nImage-Based\nTask Specific Prompt\nOutput\nTableQA\nTableQA\nVanilla-T\nRow-Identifier\nBracket\nColumn-JSON\nRow-JSON",
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8",
      "achieve a decent performance (52.5% and 60.3%\nrespectively using the vanilla prompting and lin-\nearized table representation from Table 4), demon-\nstrating their strong table understanding abilities.\nIn such cases, brackets or other kinds of table repre-\nsentations may add extra “workload” to the model,\nwhich dilutes the models’ attention to the original\ntable content and thus leads to worse performance.\nRQ3. How do different image-based prompt\nmethods affect LLMs’ performance on\ntable-related tasks?\nTest: We test the three image-based table rep-\nresentations in Section 3.2 together with vanilla,\nchain-of-thought, and expert prompting. We test\nthe Geminipro and GPT-4 model which can take im-\nages as the input. We average the accuracy scores\nacross FinQA, LogicNLG, TabFact, and WikiTQ.\nAppendix C.3 reports LLMs’ performance on E2E\nand ToTTo datasets.\nTL;DR Answer 3.1: CoT prompting helps\nLLMs reason over images of the table.\nFull Answer 3.1: In Table 5, we observe\nthat chain-of-thought prompting helps multimodal\nLLMs in all image-based table representations. For\ninstance, when using different colors to distinguish\nrows in the table (Row-Color), the average accu-\nracy score for GPT-4 improves from 51.8% by\nvanilla prompting to 60.2% by chain-of-thought\nprompting. By explicitly outlining the reason-\ning process, chain-of-thought prompting may help\nLLMs better understand the context and relation-\nships between different rows and columns in the\ntable, therefore better aligning this visual informa-\ntion with the question text. Such consistent perfor-\nmance improvements suggest that chain-of-thought\nprompting may enhance information fusion across\nthe text and vision modality.\nTL;DR Answer 3.2: Distinguishing rows may\nlead to better performance for LLMs to reason\nover images of the table.\nFull Answer 3.2: In Table 5, under CoT prompt-\ning, GPT4 performs slightly better when using col-\nors to distinguish different rows, which also yields\nthe overall best performance using images of the\ntable. In contrast, under CoT prompting, using\ncolors to distinguish columns yields similar perfor-\nmance to vanilla image (58.0% to 57.8% for GPT-4\nand 45.1% to 46.0% for Geminipro), suggesting\nthat these advanced LLMs may not capture row\ninformation as well as column information.\nFigure 6 shows a WikiTQ example with\nGeminipro model’s predictions. Since the question\nasks about the number of games, it requires the\nmodel to count how many rows satisfy such a con-\ndition. Using colors to distinguish rows may help\nmodels visually segment and categorize the data.\nThis visual differentiation may act as a cognitive\naid, which reduces the complexity of parsing and\ninterpreting the tabular data.\nTL;DR Answer 3.3: The more capable LLM\ndoes not necessarily benefit more from the col-\nored images.\nFull Answer 3.3: In addition, if we use the\nvanilla prompt, the different coloring methods may\neven hurt the performance of GPT-4 (for GPT-4,\ncoloring rows with different colors yields 51.8%\ncompared to 56.2% without adding any color), but\nhelpful for Geminipro (for Geminipro, coloring rows\nwith different colors yields 39.4% compared to\n36.8% without adding any color). This suggests\nthat the effectiveness of how different LLMs can\nleverage colored images varies, and does not de-\npend on the model’s overall performance.\nRep Cues GPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nT N/A T T V N/A V T V V V V+T 34 43 21 10 20 41\n30 51 25 14 16 37\n- 57 30 - 58 34 - 57 28 - 61 38 - - -\n- - -\n- - -\n- - -\nTable 6: Accuracy scores of LLMs on FinQA. We\nuse vanilla prompting across experiments in this table.\nGMNpro represents Geminipro model. We denote text\nand image-based table representations as “T” and “V”\nin the “Rep” column, respectively. The “Cues” column\nindicates how we highlight the relevant cells, where\n“N/A” indicates no information about relevant cells, “T”\nindicates referring to relevant cells in the text, “V” in-"
    ]
  },
  {
    "question": "Who are the authors of the paper?",
    "answer": "The authors of the paper titled \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" are Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, and Rada Mihalcea [1].",
    "references": [
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2022. PubHealthTab: A public health table-based\ndataset for evidence-based fact checking. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 1–16, Seattle, United States. As-\nsociation for Computational Linguistics.\nSyeda Nahida Akter, Zichun Yu, Aashiq Muhamed,\nTianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera,\nKrish Dholakia, Chenyan Xiong, and Graham Neu-\nbig. 2023. An in-depth look at gemini’s language\nabilities. arXiv preprint arXiv:2312.11444.\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan,\nand Phillip Isola. 2022. Exploring visual prompts\nfor adapting large-scale models. arXiv preprint\narXiv:2203.17274.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023a. A multitask, multilingual,\nmultimodal evaluation of ChatGPT on reasoning, hal-\nlucination, and interactivity. In Proceedings of the\n13th International Joint Conference on Natural Lan-\nguage Processing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 675–718,\nNusa Dua, Bali. Association for Computational Lin-\nguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023b. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nMiao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Zhou\nJingbo, Dejing Dou, and Hui Xiong. 2022. To-\nwards table-to-text generation with pretrained lan-\nguage model: A table structure understanding and\ntext deliberating approach. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 8199–8210, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4)."
    ]
  },
  {
    "question": "What is the title of the paper?",
    "answer": "The title of the paper is \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" [1].",
    "references": [
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2022. PubHealthTab: A public health table-based\ndataset for evidence-based fact checking. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 1–16, Seattle, United States. As-\nsociation for Computational Linguistics.\nSyeda Nahida Akter, Zichun Yu, Aashiq Muhamed,\nTianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera,\nKrish Dholakia, Chenyan Xiong, and Graham Neu-\nbig. 2023. An in-depth look at gemini’s language\nabilities. arXiv preprint arXiv:2312.11444.\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan,\nand Phillip Isola. 2022. Exploring visual prompts\nfor adapting large-scale models. arXiv preprint\narXiv:2203.17274.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023a. A multitask, multilingual,\nmultimodal evaluation of ChatGPT on reasoning, hal-\nlucination, and interactivity. In Proceedings of the\n13th International Joint Conference on Natural Lan-\nguage Processing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 675–718,\nNusa Dua, Bali. Association for Computational Lin-\nguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023b. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nMiao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Zhou\nJingbo, Dejing Dou, and Hui Xiong. 2022. To-\nwards table-to-text generation with pretrained lan-\nguage model: A table structure understanding and\ntext deliberating approach. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 8199–8210, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.",
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–"
    ]
  },
  {
    "question": "What is the abstract of the paper?",
    "answer": "The abstract of the paper titled \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" states that the study investigates the effectiveness of various large language models (LLMs) in interpreting tabular data through different prompting strategies and data formats. It extends analyses across six benchmarks for table-related tasks, such as question-answering and fact-checking. The paper introduces the assessment of LLMs’ performance on image-based table representations for the first time and compares five text-based and three image-based table representations, demonstrating the role of representation and prompting on LLM performance. The study aims to provide insights into effectively utilizing LLMs for table-related tasks [1].",
    "references": [
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,"
    ]
  },
  {
    "question": "What is the introduction of the paper?",
    "answer": "The introduction of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" discusses the rapid growth of Large Language Models (LLMs) and their impressive performance on various Natural Language Processing (NLP) tasks. However, it highlights that the effectiveness of these models on structured data, particularly tabular information, has been less thoroughly explored. Unlike unstructured text, tables represent information in a systematic organization, making them foundational for numerous applications such as medical diagnostics and customer relationship management.\n\nThe introduction identifies several challenges in evaluating LLMs' performance with tabular data, such as the multitude of ways to represent table information (like naive linearization or brackets) and the diverse prompting methods applicable to tabular tasks. It establishes the importance of examining model performance on tasks like table fact-checking, question answering, and table-to-text generation, which necessitate a comprehensive understanding of table structures by LLMs. Ultimately, the paper outlines its objective to systematically evaluate various LLMs' performance on tabular information processing using both textual and image representations, while also investigating different prompting strategies to optimize this process [1].",
    "references": [
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "Wu et al., 2023; Zheng et al., 2024). On the con-\ntrary, to the best of our knowledge, few works have\nprompted these LLMs on tasks involving tabular\ndata.\nFor closed-source LLMs, researchers adopt hard\nprompts to manually craft text prompts with dis-\ncrete tokens (Qiao et al., 2022; Bahng et al., 2022;\nLiu et al., 2023). Wei et al. (2022) develop chain-\nof-though prompting, Xu et al. (2023a) develop\nexpert prompting. In our work, we include the\ncomparison between vanilla, chain-of-thought, and\nexpert prompting for LLMs on table-related tasks.\n3 Experiment Setups\n3.1 Experimented LLMs\nTable 1 describes the LLMs we use for our experi-\nments. We use closed-source models such as GPT-\n3.5 and GPT-4 (Brown et al., 2020; Ouyang et al.,\n2022), and Gemini (Team et al., 2023). We note\nthat GPT-4 and Gemini are multimodal models,\nwhich can take tables as images. For open-source\nmodels, we use the chat models from Llama-2 (Tou-\nvron et al., 2023) families from the 7 billion to the\n70 billion parameter version as they are claimed\nto perform on par with closed-source models like\nChatGPT.*\n*https://huggingface.co/meta-Llama/Llama-2-70b-chat\nMethod Name Table Representation\nVanilla-T c1, c2, · · · , cn, v(1,1), v(1,2), · · · , v(1,n), v(2,1), v(2,2),\n· · · , v(2,n), · · · , v(m,1), v(m,2), · · · , v(m,n).\nRow-Identifier\nc1, c2, · · · , cn, [ROW1] v(1,1), v(1,2), · · · , v(1,n),\n[ROW2] v(2,1), v(2,2), · · · , v(2,n), · · · , [ROWm]\nv(m,1), v(m,2), · · · , v(m,n).\nBracket [ [c1, c2, · · · , cn], [v(1,1), v(1,2), · · · , v(1,n)], [v(2,1),\nv(2,2), · · · , v(2,n)], · · · , [v(m,1), v(m,2), · · · , v(m,n)]].\nColumn-JSON { c1: [v(1,1), v(2,1), · · · , v(m,1)], c2: [v(1,2), v(2,2), · · ·\n, v(m,2)], · · · , c\nn: [v(1,n), v(2,n), · · · , v(m,n)] }.\nRow-JSON\n[{ Row: 1, c1: v(1,1), c2: v(1,2), · · · , c\nn\n: v(1,n)}, { Row: 2,\nc1: v(2,1), c2: v(2,2), · · · , c\nn\n: v(2,n) }, · · · , { Row: m, c1:\nv(m,1), c2: v(m,2), · · · , c\nn\n: v(m,n) }].\nTable 2: Text-based table representation examples. We\nconstruct the examples assuming a table of m rows\nand n columns, where ci denotes the column name of\ncolumn i and v(i,j) denotes the cell value at row i and\ncolumn j. We use colored text to indicate different rows\nin the table to assist readers.\n3.2 Prompting Strategies\nWe explore two ways to represent tables in the\nprompt, Text-Based and Image-Based.",
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,"
    ]
  },
  {
    "question": "What is the methodology of the paper?",
    "answer": "The methodology of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" consists of several key components designed to evaluate the performance of different large language models (LLMs) on tabular data. This includes:\n\n1. **Experiment Setup**: The study involves systematic comparisons across different models, including both closed-source (e.g., GPT-3.5, GPT-4, and Gemini) and open-source models (e.g., Llama-2). The models are assessed on their ability to handle tabular data presented in various formats (text-based and image-based) [1].\n\n2. **Table Representations**: The methodology specifies multiple ways to represent tabular data, including linearized text representations and structured formats such as JSON. Each representation is designed to test how effectively the models interpret the underlying data [1].\n\n3. **Prompting Strategies**: Various prompting techniques are compared, including vanilla prompting, chain-of-thought prompting, and expert prompting. The effect of these strategies on the models' ability to perform tasks related to tables (like question answering and fact-checking) is a significant part of the evaluation [2].\n\n4. **Performance Metrics**: Model performance is measured using accuracy scores and ROUGE-L scores for generated outputs. This involves both automated metrics and manual evaluations where the generated sentences are assessed for completeness and correctness relative to the table data [3].\n\n5. **Statistical Tests**: The methodology incorporates statistical tests to determine the significance of performance differences between models when presented with different types of input (text vs. image) and prompting strategies. This helps validate the findings and conclusions drawn from the experimental results [4].\n\nThis comprehensive investigative approach aims to provide insights into the capabilities of LLMs when it comes to processing, reasoning, and generating information based on tabular data [5].",
    "references": [
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8",
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,"
    ]
  },
  {
    "question": "What is the related work of the paper?",
    "answer": "The related work section of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" identifies various previous studies and advancements in the field of language processing related to tables. It acknowledges the existing literature that has evaluated Large Language Models (LLMs) and highlights how their effectiveness has primarily been demonstrated on unstructured data while tabular information remains less explored.\n\nA significant reference is made to recent efforts that specifically tackle table-based tasks, such as the development of datasets like FinQA, LogicNLG, TabFact, and WikiTQ, which focus on numerical reasoning and fact verification within tabular contexts. The related work also points out that most research has focused on hard prompts crafted to interact with LLMs using discrete tokens. Innovations like chain-of-thought and expert prompting have emerged to enhance model performance on varied tasks but have often not been explicitly tested in the context of tabular data.\n\nMoreover, the authors note works dedicated to pre-training models for specific tasks involving tables, indicating a growing interest in the area but also establishing a gap that their paper aims to fill by systematically evaluating LLMs on table-related tasks using both textual and image-based representations [1].",
    "references": [
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "dicates highlighting relevant cells on the table image,\n“V + T” indicates both highlighting relevant cells on the\ntable image and referring to them in the text.\nRQ4. Does highlighting relevant cells yield a\nbetter performance?\nTest: We test all six LLMs in Section 3.1 on\nFinQA which provides relevant cells in the table\nfor each instance. We refer to the relevant cells\nby adding “Please pay attention to the highlighted\ncells: (row index, column index, cell value)” in the\ntext prompt, or mark them on the table image di-\nrectly. Appendix E provides our prompt examples.\nWe use vanilla prompting in this comparison.\nTL;DR Answer: Yes.\nFull Answer: In Table 6, we notice that in most\ncases, referring LLMs to specific cells helps LLMs\nbetter attend to them, thereby helping LLMs reason\nover the example. However, LLMs’ performance\nmay get hurt when we refer to the relevant cells\nthrough text such as Llama-2-13B and 70B. This\nmay be due to the inherent limitations of textual\ndescriptions for conveying spatial or relational in-\nformation. In order to relate the mentioned cells in\nthe text, the model needs to figure out the connec-\ntion between the mentioned cell and the cell in the\nlinearized table, which can be challenging to the\nmodel given the complicated table structure.\nIn addition, LLMs best attend to the table items\nwhen there are clues from both text and image. In\nTable 6, we observe that marking the relevant cells\non the image while mentioning them through text\nleads to the most correctly answered examples (61\nexamples by GPT-4 and 38 by Geminipro at the last\nrow in Table 6). Such a dual-modality approach\nthat combines visual cues with text references, en-\n∆ Metric\nFinQA 47.0 57.0 +10.0\nLogicNLG 43.4 58.5 +15.1\nTabFact 51.8 74.7 +22.9\nWikiTQ 69.0 86.0 +17.0\nAcc\nE2E 37.1 46.0 + 8.9 ROUGE-L\nToTTo 30.1 47.7 +17.6\nTable 7: Performance scores of the best performed\nopen-source ( ) LLM we test, Llama-2-70B versus\nclosed-source ( ) LLM we test, GPT-4 on different\ndatasets. The closed-source LLMs always outperform\nthe open-source LLMs and we report the performance\ndifference ∆ between them. For consistency across dif-\nferent datasets, we do not include the performances\nwith highlighting cells*in this comparison. Table 20 in\nAppendix D provides what combinations of table repre-\nsentation and prompting method yield this performance.\nDate Country Placing …\n10/31/2008 UK 1st …\n…\nQuestion: What is the number of 1st\nplace finishes across all events?\nGold: 17\nGPT-4: 15 ⨯\nFigure 7: An example from WikiTQ where GPT-4 can-\nnot answer it correctly with bracket table representation\nand chain-of-thought prompting. In addition, for most\nof the table representations and prompting styles, GPT-4\ncannot answer this question correctly.\nhances LLMs’ overall reasoning ability over the\ntabular data.\nRQ5 and RQ6\nWe include two additional research questions and\nour answer to them in Appendix C, including\nwhether these LLMs can reconstruct tables from\nthe image, and whether multimodal LLMs benefit\nfrom having both formats simultaneously as input.\n5 Open Problems to Increase the\nPerformance of LLMs on Tabular Data\nMathematical reasoning. We observe that LLMs\nare not good at arithmetic reasoning similar to the\nfindings in prior works (Hendrycks et al., 2021;\nImani et al., 2023). As shown in Figure 7, sim-\nple arithmetic computing like counting the total\nnumber of rows that satisfy certain conditions (‘1st’\nin Figure 7) still poses challenges even for GPT-4.\n*Except for ToTTo, where the task is to generate the sen-\ntence based on the highlighted cells. On ToTTo, we include\nthe highlight information just in text.\nQuestion: Write a sentence to describe the table.\nName EatType Area\nBlue Spice pub Riverside\nModel Generated\nThe table contains three columns and three rows, with\nL-7B\nL-13B\nthe data “Blue Spice”, “pub”, and “riverside” in the\nfirst column, respectively.\n⨯\nThe table contains information about three different\nrestaurants, including their names, eat types (pub or\nrestaurant), and locations (riverside or not)."
    ]
  },
  {
    "question": "What is the conclusion of the paper?",
    "answer": "The conclusion of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" summarizes the key findings from the study, highlighting the effectiveness of various LLMs in interpreting tabular data across different prompting strategies and representation formats. The paper emphasizes that both text-based and image-based representations of tables can yield competent performance from LLMs, with the image representations sometimes improving model outcomes. \n\nAdditionally, it discusses the nuances revealed in how different prompting approaches, such as vanilla, chain-of-thought, and expert prompting, significantly affect the performance of LLMs on table-related tasks. The authors argue that their research provides valuable insights into optimizing the use of LLMs for processing tabular data in applications ranging from question answering to fact-checking. The conclusion also identifies avenues for future work, including further exploration of how LLMs can better manage mathematical reasoning in the context of tables and improving model interaction with tabular data through advanced prompting and representation strategies [1].",
    "references": [
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2022. PubHealthTab: A public health table-based\ndataset for evidence-based fact checking. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 1–16, Seattle, United States. As-\nsociation for Computational Linguistics.\nSyeda Nahida Akter, Zichun Yu, Aashiq Muhamed,\nTianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera,\nKrish Dholakia, Chenyan Xiong, and Graham Neu-\nbig. 2023. An in-depth look at gemini’s language\nabilities. arXiv preprint arXiv:2312.11444.\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan,\nand Phillip Isola. 2022. Exploring visual prompts\nfor adapting large-scale models. arXiv preprint\narXiv:2203.17274.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023a. A multitask, multilingual,\nmultimodal evaluation of ChatGPT on reasoning, hal-\nlucination, and interactivity. In Proceedings of the\n13th International Joint Conference on Natural Lan-\nguage Processing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 675–718,\nNusa Dua, Bali. Association for Computational Lin-\nguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023b. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nMiao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Zhou\nJingbo, Dejing Dou, and Hui Xiong. 2022. To-\nwards table-to-text generation with pretrained lan-\nguage model: A table structure understanding and\ntext deliberating approach. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 8199–8210, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4)."
    ]
  },
  {
    "question": "What is the future work of the paper?",
    "answer": "The future work section of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" discusses several potential areas for further research:\n\n1. **Improving Mathematical Reasoning**: The authors observed that LLMs, particularly GPT-4, struggle with arithmetic reasoning tasks that involve counting or performing calculations based on tabular data. Future research can focus on enhancing LLMs' capabilities in this area to ensure more accurate results when faced with numerical reasoning tasks [1].\n\n2. **Advanced Prompting Techniques**: The paper suggests exploring more sophisticated techniques beyond those currently tested, such as refining prompting strategies that might further improve the models' performance on tabular data tasks. This may include developing new methods to assist LLMs in understanding the connections between query text and tabular structures more effectively [2].\n\n3. **Multimodal Interactions**: Further investigation into how multimodal LLMs can benefit from being presented with both text and image representations concurrently is another suggested area. Understanding the best approaches to utilize both types of data simultaneously could improve the models' reasoning abilities and accuracy in handling complex tabular information [3].\n\n4. **Dataset Expansion**: The authors indicate a need to gather more diverse datasets specifically designed for testing the reasoning capabilities of LLMs with tabular information. This could help generalize findings across different contexts and applications [4].\n\nIn conclusion, the authors emphasize the importance of addressing these areas to advance the understanding and practical application of LLMs in processing tabular data, particularly for real-world applications in diverse fields [5].",
    "references": [
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,",
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–"
    ]
  }
]
