[
  {
    "question": "What are the main objectives of the study on Table Information Seeking (TIS) in Large Language Models (LLMs)?",
    "answer": "The main objectives of the study on Table Information Seeking (TIS) in Large Language Models (LLMs) are to investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats, and to evaluate their performance on six benchmarks for table-related tasks, including question-answering and fact-checking. The study also aims to assess the performance of LLMs with image-based table representations for the first time and to analyze how different representation and prompting methods influence the performance of LLMs on table-related tasks [1].",
    "references": [
      "arxiv:2402.12424v5 [cs.lg] 17 oct 2024\ntables as texts or images:\nevaluating the table reasoning ability of llms and mllms\nnaihao deng* , zhenjie sun* , ruiqi he , aman sikka ,\nyulong chen , lin ma , yue zhang , rada mihalcea\nuniversity of michigan university of cambridge westlake university\n{dnaihao, zjsun}@umich.edu\nabstract\nin this paper, we investigate the effectiveness\nof various llms in interpreting tabular data\nthrough different prompting strategies and data\nformats. our analyses extend across six bench-\nmarks for table-related tasks such as question-\nanswering and fact-checking. we introduce\nfor the first time the assessment of llms’\nperformance on image-based table represen-\ntations. specifically, we compare five text-\nbased and three image-based table represen-\ntations, demonstrating the role of representa-\ntion and prompting on llm performance. our\nstudy provides insights into the effective use\nof llms on table-related tasks. our data is\navailable at: https://github.com/dnaihao/\ntables-as-texts-or-images.\n1 introduction\nrecent years have witnessed an explosion of large\nlanguage models (llms), with impressive per-\nformance on various natural language processing\n(nlp) tasks (brown et al., 2020; touvron et al.,\n2023; team et al., 2023). research to date has\nexamined the performance of llms for various\naspects and abilities (bang et al., 2023b; bubeck\net al., 2023; akter et al., 2023), but their effec-\ntiveness on structured data such as tables is less\nexplored.\nunlike unstructured text, tables are systemat-\nically organized structures of a large amount of\ninformation. this characteristic makes tabular\ndata serve as the foundations for numerous ap-\nplications, including medical diagnostics, virtual\npersonal assistants, customer relationship manage-\nment (hemphill et al., 1990; dahl et al., 1994;\nakhtar et al., 2022; xie et al., 2022), etc.\nthe evaluation of llms on processing tabular\ndata involves many challenges. first, there are\nmany ways to represent the information in tables.\n*contributed equally to this work. appendix a lists the\ndetailed contributions.\nif we represent the table in pure text, we may use\nnaive linearization or insert brackets to better rep-\nresent table structures. meanwhile, emerging mul-\ntimodal llms like gpt-4 (achiam et al., 2023)\nand gemini (team et al., 2023) offer image-based\napproaches, where we can pass the table as im-\nages to the llms. in such cases, visual cues like\ncolor highlighting in tables can influence outcomes.\nsecond, diverse prompting methods for text may\nalso apply to tabular data, which can yield varied\nresults (wei et al., 2022). furthermore, the tasks\ninvolving tabular data are diverse, including table\nfact-checking (chen et al., 2019) and table question\nanswering (pasupat and liang, 2015), and table-to-\ntext generation (novikova et al., 2017), etc.\nin this paper, we systematically evaluate model\nperformance on tabular data for both textual llms\nand multi-modal llms. specifically, we investi-\ngate several research questions, including the ef-\nfectiveness of image-based representation of tab-\nular data and how different text-based or image-\nbased prompt methods affect llms’ performance\non table-related tasks. in addition, we provide anal-\nysis and hypothesis of llms’ behaviors. our find-\nings include:\n• llms maintain decent performance when we\nuse image-based table representations. some-\ntimes, image-based table representations can\nmake llms perform better.\n• there are nuances in the prompting design for\ntable-related tasks, revealed by our comparisons\nof various prompting methods for text- and\nimage-based table representations.\nto the best of our knowledge, we are the first to\nstudy how llms perform with image-based table\nrepresentations. we believe this paper draws new\ninsights into optimizing table-based information\nprocessing.\ntext-based\nimage-based\ntask specific prompt\noutput\ntableqa\ntableqa\nvanilla-t\nrow-identifier\nbracket\ncolumn-json\nrow-json",
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect",
      "natural language and code for language agents. arxiv\npreprint arxiv:2310.06830.\njingfeng yang, aditya gupta, shyam upadhyay,\nluheng he, rahul goel, and shachi paul. 2022.\ntableformer: robust transformer modeling for table-\ntext encoding. in proceedings of the 60th annual\nmeeting of the association for computational lin-\nguistics (volume 1: long papers), pages 528–537,\ndublin, ireland. association for computational lin-\nguistics.\nzhengyuan yang, linjie li, kevin lin, jianfeng\nwang, chung-ching lin, zicheng liu, and lijuan\nwang. 2023. the dawn of lmms: preliminary\nexplorations with gpt-4v (ision). arxiv preprint\narxiv:2309.17421, 9(1):1.\npengcheng yin, graham neubig, wen-tau yih, and se-\nbastian riedel. 2020. tabert: pretraining for joint\nunderstanding of textual and tabular data. in proceed-\nings of the 58th annual meeting of the association\nfor computational linguistics, pages 8413–8426, on-\nline. association for computational linguistics.\nliangyu zha, junlin zhou, liyao li, rui wang, qingyi\nhuang, saisai yang, jing yuan, changbao su, xiang\nli, aofeng su, et al. 2023. tablegpt: towards unify-\ning tables, nature language and commands into one\ngpt. arxiv preprint arxiv:2307.08674.\nhongzhi zhang, yingyao wang, sirui wang, xuezhi\ncao, fuzheng zhang, and zhongyuan wang. 2020.\ntable fact verification with structure-aware trans-\nformer. in proceedings of the 2020 conference on\nempirical methods in natural language processing\n(emnlp), pages 1624–1629, online. association for\ncomputational linguistics.\ntianshu zhang, xiang yue, yifei li, and huan sun.\n2023. tablellama: towards open large generalist\nmodels for tables.\nboyuan zheng, boyu gou, jihyung kil, huan sun, and\nyu su. 2024. gpt-4v (ision) is a generalist web agent,\nif grounded. arxiv preprint arxiv:2401.01614.\na contributions\nidea proposal. naihao deng proposed the idea\nof evaluating llms’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nbackground survey. zhenjie sun conducted a\ncareful survey on table-related tasks. naihao deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in westlake\nuniversity with yue zhang.\nimplementation. zhenjie sun came up with var-\nious table representations and designed all the\nprompts. zhenjie sun also implemented the eval-\nuation pipeline for autonomous metrics. naihao\ndeng and zhenjie sun implemented the pipeline\nfor human evaluation.\nexperiments. zhenjie sun and naihao deng\nconducted all the experiments. specifically,\nzhenjie sun conducted experiments on gpt-3.5,\ngpt-4 with text-based table representations, and\ngeminipro. naihao deng conducted experiments\non llama-2 models and gpt-4 with image-based\ntable representations.\nresult aggregation. zhenjie sun conducted the\nresult aggregation for all the experiments.\nhuman evaluation. zhenjie sun, naihao deng,\nruiqi he, aman sikka conducted the human evalu-\nation for the model generation.\npaper writing. naihao deng drafted the paper.\nzhenjie sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. naihao deng and zhenjie sun plot-\nted all the tables and figures. naihao deng, zhenjie\nsun selected examples that appeared in this paper.\nruiqi he, aman sikka provided suggestions on\nexample selections. rada mihalcea, yue zhang,"
    ]
  },
  {
    "question": "How does the newly introduced benchmark, TabIS, differ from previous evaluation methods for table information extraction?, ",
    "answer": "The newly introduced benchmark, TabIS, differs from previous evaluation methods for table information extraction by specifically assessing the performance of large language models (LLMs) on interpreting tabular data with a focus on various representations (text-based and image-based) and diverse prompting strategies. Unlike traditional benchmarks, TabIS aims to systematically evaluate how well LLMs can handle structured data, highlighting the effectiveness of image-based representations and the nuances involved in prompting for table-related tasks [1].",
    "references": [
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect",
      "llms’ performance on tabular-related\ntasks?\ntables 12 and 13 report the rouge-l scores of\ngpt-4 and geminipro when we use image repre-\nsentations of tables on e2e and totto dataset,\nrespectively. table 14 reports the scores annotated\nmanually by the authors.\nc.4 rq5: can these llms reconstruct\ntables from the image?\ntest: we conduct experiments to explore table re-\nconstruction ideas using gpt4 on e2e and finqa\ndatasets.\ntl;dr answer: mostly yes.\nfull answer: gpt4 successfully reconstructs\n97/100 tables on the e2e dataset, and 72/100 tables\non the finqa dataset. we notice that for simpler ta-\nbles, gpt4 can reconstruct tables almost perfectly.\nfor instance, in e2e, the only “mistake” the model\nmakes is to output “é” instead of “e” in “rainbow\nvegetarian café”.\ngpt4 also demonstrates a decent capability of\nreconstructing complicated tables. on finqa,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8\n53.0 54.1 50.0\n46.0 46.3\n47.0 52.1\n41.3\n58.0\n65.0\n36.0 44.5\n45.5\n44.6 43.5\n38.0\n38.3 43.4\n21.1\ngmnpro(t)\ngmnpro(v)\ngpt-4 (t)\ngpt-4 (v)\nfinqa logicnlg tabfact wikitq e2e totto\nfigure 9: performance comparison between passing the text versus image representations of tables to gpt-4 and\ngeminipro across finqa, logicnlg, tabfact, and wikitq by accuracy, and e2e and totto by rouge-l scores.\nwe use the linearized table (vanilla-t) as the text-based representation, the original table image (vanilla-v) as the\nimage-based representation, and cot prompting.\n80\n60\n40\n20\n0\n64.0\n56.0\n72.0 77.0\n58.2 63.0\n29.0 33.0 52.0 54.1\n53.0\n30.1\n31.7\n45.3\n44.9 42.8\n42.4\n45.0\n42.3\n39.2\n35.9\n35.6\n35.0 28.0\ngmnpro(t)\ngmnpro(v)\ngpt-4 (t)\ngpt-4 (v)\nfinqa logicnlg tabfact wikitq e2e totto\nfigure 10: performance comparison between passing the text versus image representations of tables to gpt-4 and\ngeminipro across finqa, logicnlg, tabfact, and wikitq by accuracy, and e2e and totto by rouge-l scores.\nwe use the linearized table (vanilla-t) as the text-based representation, the original table image (vanilla-v) as the\nimage-based representation, and expert prompting.\ngpt-4 manages to reconstruct a 4-row×10-column\ntable with over 100 words (table 16).\nbut gpt-4 is more prone to hallucination or\nmessing up with the spatial relations when the table\ngets more complicated. for instance, for table 17,\ngpt-4 reconstructs it as table 18. as expected,\ngpt-4 fails to answer the corresponding question\nto this table when using a table image as the in-\nput. however, we notice that when using the text\nrepresentation of the table, gpt-4 also fails to an-\nswer this question. this aligns with what we have\nreported in.\nas expected, gpt-4 fails to answer the corre-\nsponding question to this table when using a table\nimage as the input. however, we notice that when\nusing the text representation of the table, gpt-4\nalso fails to answer this question. this aligns with\nwhat we have reported in figure 3 that on finqa,\ngpt-4 better leverages the image representations\nthan the text representations in general.\nc.5 rq6: do multimodal llms benefit from\nhaving both formats simultaneously as\ninput?\ntest: the six datasets.\nwe test gpt-4 with vanilla prompting for\ntl;dr answer: not generally true.\nfull answer: as shown in table 19, it is not",
      "the table contains information about three different\nrestaurants, including their names, eat types (pub or\nrestaurant), and locations (riverside or not).\nl-70b\nthe table shows the name, eattype, and area of a\nrestaurant, with one entry for blue spice, a pub\nlocated in the riverside area.\n⨯\n✓\nfigure 8: table-to-text generation from e2e. we\nuse vanilla prompting and bracket table representa-\ntion across all the models. “l-7b/13b/70b” represents\nllama-2-7b/13b/70b, respectively. we highlight the\npart where the model generates incorrectly in red.\nthis suggests that these previously proposed bench-\nmarks are still valuable in evaluating llms, as\nmany of these datasets involve arithmetic reasoning\nsuch as finqa and wikitq, which is challenging\nfor even the most sophisticated llms like gpt-4.\nfor datasets like e2e for table-to-text generation,\nthough relatively easy for advanced models like\ngpt-4, smaller models cannot describe the table\nfully correctly as shown in figure 8. therefore,\nthese datasets are still valuable for benchmarking\nthe progress of smaller llms.\nappendix f provides more examples from each\ndataset where llms can and cannot answer well.\nclosing the gap between open-source and closed-\nsource llms in table 7, we report the best per-\nformance among different prompting methods for\nthe best performed open-source llama-2 models\nversus the best performed closed-source gpt mod-\nels on finqa, logicnlg, tabfact, and wikitq.\nwe note that on these tasks, gpt-4 always performs\nthe best among all the closed-source llms we test.\nwe observe that open-source llama-2 models\nobtain decent performances across these bench-\nmarks. as shown in table 20. figure 8 shows an\nexample from e2e dataset. the smaller llama-2\nmodels such as llama-2-7b or llama-2-13b make\nmistakes in counting rows. however, they capture\nalmost all the information in the table including\nthe name, eat type, and area of the restaurant. as\nthe model scales up, the llama-2 70b model can\ndescribe the table accurately.\nhowever, significant performance gaps exist be-\ntween open-source llama-2 models and closed-\nsource gpt-4 models. in table 7, the gap between\nopen-source llama-2 models and gpt-4 can be\nas large as 15% on finqa and 22.9% on tabfact.\neven on logicnlg which has the smallest perfor-\nmance gap, there is an 8.4% difference between the\nllama-2 and gpt models. as llama models often\nserve as the foundation models for a wide range\nof nlp research (roziere et al., 2023; xu et al.,\n2023b), we need the effort from the open-source\ncommunity to keep developing stronger llms to\nclose the gap between open-source and closed-\nsource llms.\n6 conclusion\nwe have explored various representation strategies,\nincluding both text-based and innovative image-\nbased approaches, to understand how to use llms\neffectively in tasks involving tabular data. we\ndemonstrate the effectiveness of image-based rep-\nresentations and reveal the impact of prompting\nstrategies on the performance of llms. we be-\nlieve our insights contribute to the understanding\nof llms and how to optimize llms for tabular\ndata processing.\n7 ethical statement\nwe conduct our studies on six pre-existing and\npublically available datasets using various existing\nllms. prior works have pointed out the potential\nbias in these llms (bender et al., 2021) which\npractitioners need to be aware of.\n8 limitations\nin this study, we do not intend to exhaust every\npossible text representation, image representation\nof tables, or every possible llm. moreover, we do\nnot have access to the closed-source llms behind\ntheir api. instead, we designed our experiments to\ntry to explore the research questions we raised in\nthis paper. we hope our findings and insights in this\npaper can inspire future research on table-related\ntasks.\nacknowledgements\nwe thank inderjeet nair for the early discussion\non different forms of table representations; alexan-\nder hanbo li, and patrick ng for discussions of\nour work and pointers to related work; the authors\nof the table-related datasets we use in our exper-\niments for making their datasets publically avail-\nable; the anonymous reviewers for their valuable\nsuggestions. this project was partially supported\nby the openai researcher access program and by"
    ]
  },
  {
    "question": "What were the performance results of various LLMs tested using the TabIS benchmark, particularly regarding their understanding of table structures?\n",
    "answer": "The performance results of various LLMs tested using the TabIS benchmark indicate a notable discrepancy in their understanding of table structures when employing different prompting methods. For instance, GPT-4 demonstrated superior performance with an accuracy of 88.0 on the WikitQ dataset using text representations (vanilla-t), while the image representation (vanilla-v) yielded a slightly lower accuracy of 82.0. In the TabFact benchmark, GPT-4 achieved an accuracy of 69.7 with text input and 70.4 with image input, showing its ability to process both formats effectively.\n\nHowever, performance varied significantly across different models; for example, the LLaMA-3 model scored only 43.0 on WikitQ and 44.0 on FinQA, highlighting its lower capability in understanding tables compared to GPT-4. Meanwhile, models like Gemma and the smaller LLaMA versions demonstrated much lower accuracies across the benchmarks, suggesting that larger, more advanced models perform significantly better on table-related tasks [1].",
    "references": [
      "arxiv:2402.12424v5 [cs.lg] 17 oct 2024\ntables as texts or images:\nevaluating the table reasoning ability of llms and mllms\nnaihao deng* , zhenjie sun* , ruiqi he , aman sikka ,\nyulong chen , lin ma , yue zhang , rada mihalcea\nuniversity of michigan university of cambridge westlake university\n{dnaihao, zjsun}@umich.edu\nabstract\nin this paper, we investigate the effectiveness\nof various llms in interpreting tabular data\nthrough different prompting strategies and data\nformats. our analyses extend across six bench-\nmarks for table-related tasks such as question-\nanswering and fact-checking. we introduce\nfor the first time the assessment of llms’\nperformance on image-based table represen-\ntations. specifically, we compare five text-\nbased and three image-based table represen-\ntations, demonstrating the role of representa-\ntion and prompting on llm performance. our\nstudy provides insights into the effective use\nof llms on table-related tasks. our data is\navailable at: https://github.com/dnaihao/\ntables-as-texts-or-images.\n1 introduction\nrecent years have witnessed an explosion of large\nlanguage models (llms), with impressive per-\nformance on various natural language processing\n(nlp) tasks (brown et al., 2020; touvron et al.,\n2023; team et al., 2023). research to date has\nexamined the performance of llms for various\naspects and abilities (bang et al., 2023b; bubeck\net al., 2023; akter et al., 2023), but their effec-\ntiveness on structured data such as tables is less\nexplored.\nunlike unstructured text, tables are systemat-\nically organized structures of a large amount of\ninformation. this characteristic makes tabular\ndata serve as the foundations for numerous ap-\nplications, including medical diagnostics, virtual\npersonal assistants, customer relationship manage-\nment (hemphill et al., 1990; dahl et al., 1994;\nakhtar et al., 2022; xie et al., 2022), etc.\nthe evaluation of llms on processing tabular\ndata involves many challenges. first, there are\nmany ways to represent the information in tables.\n*contributed equally to this work. appendix a lists the\ndetailed contributions.\nif we represent the table in pure text, we may use\nnaive linearization or insert brackets to better rep-\nresent table structures. meanwhile, emerging mul-\ntimodal llms like gpt-4 (achiam et al., 2023)\nand gemini (team et al., 2023) offer image-based\napproaches, where we can pass the table as im-\nages to the llms. in such cases, visual cues like\ncolor highlighting in tables can influence outcomes.\nsecond, diverse prompting methods for text may\nalso apply to tabular data, which can yield varied\nresults (wei et al., 2022). furthermore, the tasks\ninvolving tabular data are diverse, including table\nfact-checking (chen et al., 2019) and table question\nanswering (pasupat and liang, 2015), and table-to-\ntext generation (novikova et al., 2017), etc.\nin this paper, we systematically evaluate model\nperformance on tabular data for both textual llms\nand multi-modal llms. specifically, we investi-\ngate several research questions, including the ef-\nfectiveness of image-based representation of tab-\nular data and how different text-based or image-\nbased prompt methods affect llms’ performance\non table-related tasks. in addition, we provide anal-\nysis and hypothesis of llms’ behaviors. our find-\nings include:\n• llms maintain decent performance when we\nuse image-based table representations. some-\ntimes, image-based table representations can\nmake llms perform better.\n• there are nuances in the prompting design for\ntable-related tasks, revealed by our comparisons\nof various prompting methods for text- and\nimage-based table representations.\nto the best of our knowledge, we are the first to\nstudy how llms perform with image-based table\nrepresentations. we believe this paper draws new\ninsights into optimizing table-based information\nprocessing.\ntext-based\nimage-based\ntask specific prompt\noutput\ntableqa\ntableqa\nvanilla-t\nrow-identifier\nbracket\ncolumn-json\nrow-json",
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect",
      "and totto) uses rouge-l as metrics. the closed-source llms always outperform the open-source llms and\nwe report the performance difference ∆ between them. we include the table representation (“table repr”) and\nprompting methods that yield the best performance next to the columns that report open-source and closed-source\nllm scores, respectively. for consistency across different datasets, we do not include the performances with\nhighlighting cells in this comparison.\ngpt-4o llama-3 gemma metric\nvanilla-t vanilla-v 8b 70b 2b 7b\nwikitq 88.0 82.0 43.0 69.0 20.0 34.0\ntabfact 69.7 70.4 48.5 71.9 30.2 41.1\nlogicnlg 54.2 54.6 25.8 31.1 15.1 20.1\nfinqa 71.0 68.0 44.0 53.0 2.0 6.0\nacc\ntotto 44.0 44.8 8.8 45.6 13.5 27.2 rouge-l\ne2e 44.2 43.7 6.6 18.1 17.7 18.6\ntable 21: results for vanilla prompting gpt4-o, llama-3, and gemma"
    ]
  },
  {
    "question": "Who are the authors of the paper?",
    "answer": "The authors of the paper are Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, and Rada Mihalcea [1].",
    "references": [
      "a microsoft foundational model grant.\nreferences\njosh achiam, steven adler, sandhini agarwal, lama\nahmad, ilge akkaya, florencia leoni aleman,\ndiogo almeida, janko altenschmidt, sam altman,\nshyamal anadkat, et al. 2023. gpt-4 technical report.\narxiv preprint arxiv:2303.08774.\nmubashara akhtar, oana cocarascu, and elena simperl.\n2022. pubhealthtab: a public health table-based\ndataset for evidence-based fact checking. in find-\nings of the association for computational linguistics:\nnaacl 2022, pages 1–16, seattle, united states. as-\nsociation for computational linguistics.\nsyeda nahida akter, zichun yu, aashiq muhamed,\ntianyue ou, alex bäuerle, ángel alexander cabrera,\nkrish dholakia, chenyan xiong, and graham neu-\nbig. 2023. an in-depth look at gemini’s language\nabilities. arxiv preprint arxiv:2312.11444.\nhyojin bahng, ali jahanian, swami sankaranarayanan,\nand phillip isola. 2022. exploring visual prompts\nfor adapting large-scale models. arxiv preprint\narxiv:2203.17274.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, quyet v. do, yan xu,\nand pascale fung. 2023a. a multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. in proceedings of the\n13th international joint conference on natural lan-\nguage processing and the 3rd conference of the asia-\npacific chapter of the association for computational\nlinguistics (volume 1: long papers), pages 675–718,\nnusa dua, bali. association for computational lin-\nguistics.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, et al. 2023b. a multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arxiv\npreprint arxiv:2302.04023.\nemily m bender, timnit gebru, angelina mcmillan-\nmajor, and shmargaret shmitchell. 2021. on the\ndangers of stochastic parrots: can language models\nbe too big? in proceedings of the 2021 acm confer-\nence on fairness, accountability, and transparency,\npages 610–623.\ntom brown, benjamin mann, nick ryder, melanie\nsubbiah, jared d kaplan, prafulla dhariwal, arvind\nneelakantan, pranav shyam, girish sastry, amanda\naskell, et al. 2020. language models are few-shot\nlearners. advances in neural information processing\nsystems, 33:1877–1901.\nsébastien bubeck, varun chandrasekaran, ronen el-\ndan, johannes gehrke, eric horvitz, ece kamar,\npeter lee, yin tat lee, yuanzhi li, scott lund-\nberg, et al. 2023. sparks of artificial general intelli-\ngence: early experiments with gpt-4. arxiv preprint\narxiv:2303.12712.\nmiao chen, xinjiang lu, tong xu, yanyan li, zhou\njingbo, dejing dou, and hui xiong. 2022. to-\nwards table-to-text generation with pretrained lan-",
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect",
      "natural language and code for language agents. arxiv\npreprint arxiv:2310.06830.\njingfeng yang, aditya gupta, shyam upadhyay,\nluheng he, rahul goel, and shachi paul. 2022.\ntableformer: robust transformer modeling for table-\ntext encoding. in proceedings of the 60th annual\nmeeting of the association for computational lin-\nguistics (volume 1: long papers), pages 528–537,\ndublin, ireland. association for computational lin-\nguistics.\nzhengyuan yang, linjie li, kevin lin, jianfeng\nwang, chung-ching lin, zicheng liu, and lijuan\nwang. 2023. the dawn of lmms: preliminary\nexplorations with gpt-4v (ision). arxiv preprint\narxiv:2309.17421, 9(1):1.\npengcheng yin, graham neubig, wen-tau yih, and se-\nbastian riedel. 2020. tabert: pretraining for joint\nunderstanding of textual and tabular data. in proceed-\nings of the 58th annual meeting of the association\nfor computational linguistics, pages 8413–8426, on-\nline. association for computational linguistics.\nliangyu zha, junlin zhou, liyao li, rui wang, qingyi\nhuang, saisai yang, jing yuan, changbao su, xiang\nli, aofeng su, et al. 2023. tablegpt: towards unify-\ning tables, nature language and commands into one\ngpt. arxiv preprint arxiv:2307.08674.\nhongzhi zhang, yingyao wang, sirui wang, xuezhi\ncao, fuzheng zhang, and zhongyuan wang. 2020.\ntable fact verification with structure-aware trans-\nformer. in proceedings of the 2020 conference on\nempirical methods in natural language processing\n(emnlp), pages 1624–1629, online. association for\ncomputational linguistics.\ntianshu zhang, xiang yue, yifei li, and huan sun.\n2023. tablellama: towards open large generalist\nmodels for tables.\nboyuan zheng, boyu gou, jihyung kil, huan sun, and\nyu su. 2024. gpt-4v (ision) is a generalist web agent,\nif grounded. arxiv preprint arxiv:2401.01614.\na contributions\nidea proposal. naihao deng proposed the idea\nof evaluating llms’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nbackground survey. zhenjie sun conducted a\ncareful survey on table-related tasks. naihao deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in westlake\nuniversity with yue zhang.\nimplementation. zhenjie sun came up with var-\nious table representations and designed all the\nprompts. zhenjie sun also implemented the eval-\nuation pipeline for autonomous metrics. naihao\ndeng and zhenjie sun implemented the pipeline\nfor human evaluation.\nexperiments. zhenjie sun and naihao deng\nconducted all the experiments. specifically,\nzhenjie sun conducted experiments on gpt-3.5,\ngpt-4 with text-based table representations, and\ngeminipro. naihao deng conducted experiments\non llama-2 models and gpt-4 with image-based\ntable representations.\nresult aggregation. zhenjie sun conducted the\nresult aggregation for all the experiments.\nhuman evaluation. zhenjie sun, naihao deng,\nruiqi he, aman sikka conducted the human evalu-\nation for the model generation.\npaper writing. naihao deng drafted the paper.\nzhenjie sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. naihao deng and zhenjie sun plot-\nted all the tables and figures. naihao deng, zhenjie\nsun selected examples that appeared in this paper.\nruiqi he, aman sikka provided suggestions on\nexample selections. rada mihalcea, yue zhang,"
    ]
  },
  {
    "question": "What is the title of the paper?",
    "answer": "The title of the paper is \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" [1].",
    "references": [
      "a microsoft foundational model grant.\nreferences\njosh achiam, steven adler, sandhini agarwal, lama\nahmad, ilge akkaya, florencia leoni aleman,\ndiogo almeida, janko altenschmidt, sam altman,\nshyamal anadkat, et al. 2023. gpt-4 technical report.\narxiv preprint arxiv:2303.08774.\nmubashara akhtar, oana cocarascu, and elena simperl.\n2022. pubhealthtab: a public health table-based\ndataset for evidence-based fact checking. in find-\nings of the association for computational linguistics:\nnaacl 2022, pages 1–16, seattle, united states. as-\nsociation for computational linguistics.\nsyeda nahida akter, zichun yu, aashiq muhamed,\ntianyue ou, alex bäuerle, ángel alexander cabrera,\nkrish dholakia, chenyan xiong, and graham neu-\nbig. 2023. an in-depth look at gemini’s language\nabilities. arxiv preprint arxiv:2312.11444.\nhyojin bahng, ali jahanian, swami sankaranarayanan,\nand phillip isola. 2022. exploring visual prompts\nfor adapting large-scale models. arxiv preprint\narxiv:2203.17274.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, quyet v. do, yan xu,\nand pascale fung. 2023a. a multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. in proceedings of the\n13th international joint conference on natural lan-\nguage processing and the 3rd conference of the asia-\npacific chapter of the association for computational\nlinguistics (volume 1: long papers), pages 675–718,\nnusa dua, bali. association for computational lin-\nguistics.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, et al. 2023b. a multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arxiv\npreprint arxiv:2302.04023.\nemily m bender, timnit gebru, angelina mcmillan-\nmajor, and shmargaret shmitchell. 2021. on the\ndangers of stochastic parrots: can language models\nbe too big? in proceedings of the 2021 acm confer-\nence on fairness, accountability, and transparency,\npages 610–623.\ntom brown, benjamin mann, nick ryder, melanie\nsubbiah, jared d kaplan, prafulla dhariwal, arvind\nneelakantan, pranav shyam, girish sastry, amanda\naskell, et al. 2020. language models are few-shot\nlearners. advances in neural information processing\nsystems, 33:1877–1901.\nsébastien bubeck, varun chandrasekaran, ronen el-\ndan, johannes gehrke, eric horvitz, ece kamar,\npeter lee, yin tat lee, yuanzhi li, scott lund-\nberg, et al. 2023. sparks of artificial general intelli-\ngence: early experiments with gpt-4. arxiv preprint\narxiv:2303.12712.\nmiao chen, xinjiang lu, tong xu, yanyan li, zhou\njingbo, dejing dou, and hui xiong. 2022. to-\nwards table-to-text generation with pretrained lan-",
      "wonseok hwang, jinyeong yim, seunghyun park, and\nminjoon seo. 2019. a comprehensive exploration\non wikisql with table-aware word contextualization.\narxiv preprint arxiv:1902.01069.\nshima imani, liang du, and harsh shrivastava. 2023.\nmathprompter: mathematical reasoning using large\nlanguage models. in proceedings of the 61st an-\nnual meeting of the association for computational\nlinguistics (volume 5: industry track), pages 37–\n42, toronto, canada. association for computational\nlinguistics.\nzhijing jin, sydney levine, fernando gonzalez adauto,\nojasv kamal, maarten sap, mrinmaya sachan, rada\nmihalcea, josh tenenbaum, and bernhard schölkopf.\n2022. when to make exceptions: exploring language\nmodels as accounts of human moral judgment. in\nadvances in neural information processing systems,\nvolume 35, pages 28458–28473. curran associates,\ninc.\npengfei liu, weizhe yuan, jinlan fu, zhengbao jiang,\nhiroaki hayashi, and graham neubig. 2023. pre-\ntrain, prompt, and predict: a systematic survey of\nprompting methods in natural language processing.\nacm computing surveys, 55(9):1–35.\nqian liu, bei chen, jiaqi guo, morteza ziyadi, zeqi\nlin, weizhu chen, and jian-guang lou. 2021.\ntapex: table pre-training via learning a neural sql\nexecutor. arxiv preprint arxiv:2107.07653.\njekaterina novikova, ondˇ rej dušek, and verena rieser.\n2017. the e2e dataset: new challenges for end-\nto-end generation. in proceedings of the 18th an-\nnual sigdial meeting on discourse and dialogue,\npages 201–206, saarbrücken, germany. association\nfor computational linguistics.\nlong ouyang, jeffrey wu, xu jiang, diogo almeida,\ncarroll wainwright, pamela mishkin, chong zhang,\nsandhini agarwal, katarina slama, alex ray, et al.\n2022. training language models to follow instruc-\ntions with human feedback. advances in neural\ninformation processing systems, 35:27730–27744.\nankur parikh, xuezhi wang, sebastian gehrmann, man-\naal faruqui, bhuwan dhingra, diyi yang, and dipan-\njan das. 2020. totto: a controlled table-to-text\ngeneration dataset. in proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing (emnlp), pages 1173–1186, online. as-\nsociation for computational linguistics.\npanupong pasupat and percy liang. 2015. composi-\ntional semantic parsing on semi-structured tables. in\nproceedings of the 53rd annual meeting of the as-\nsociation for computational linguistics and the 7th\ninternational joint conference on natural language\nprocessing (volume 1: long papers), pages 1470–\n1480, beijing, china. association for computational\nlinguistics.\nshuofei qiao, yixin ou, ningyu zhang, xiang chen,\nyunzhi yao, shumin deng, chuanqi tan, fei huang,\nand huajun chen. 2022. reasoning with lan-\nguage model prompting: a survey. arxiv preprint\narxiv:2212.09597.\nalec radford, jong wook kim, chris hallacy, aditya\nramesh, gabriel goh, sandhini agarwal, girish sas-\ntry, amanda askell, pamela mishkin, jack clark,\net al. 2021. learning transferable visual models from\nnatural language supervision. in international confer-\nence on machine learning, pages 8748–8763. pmlr.\nbaptiste roziere, jonas gehring, fabian gloeckle, sten",
      "natural language and code for language agents. arxiv\npreprint arxiv:2310.06830.\njingfeng yang, aditya gupta, shyam upadhyay,\nluheng he, rahul goel, and shachi paul. 2022.\ntableformer: robust transformer modeling for table-\ntext encoding. in proceedings of the 60th annual\nmeeting of the association for computational lin-\nguistics (volume 1: long papers), pages 528–537,\ndublin, ireland. association for computational lin-\nguistics.\nzhengyuan yang, linjie li, kevin lin, jianfeng\nwang, chung-ching lin, zicheng liu, and lijuan\nwang. 2023. the dawn of lmms: preliminary\nexplorations with gpt-4v (ision). arxiv preprint\narxiv:2309.17421, 9(1):1.\npengcheng yin, graham neubig, wen-tau yih, and se-\nbastian riedel. 2020. tabert: pretraining for joint\nunderstanding of textual and tabular data. in proceed-\nings of the 58th annual meeting of the association\nfor computational linguistics, pages 8413–8426, on-\nline. association for computational linguistics.\nliangyu zha, junlin zhou, liyao li, rui wang, qingyi\nhuang, saisai yang, jing yuan, changbao su, xiang\nli, aofeng su, et al. 2023. tablegpt: towards unify-\ning tables, nature language and commands into one\ngpt. arxiv preprint arxiv:2307.08674.\nhongzhi zhang, yingyao wang, sirui wang, xuezhi\ncao, fuzheng zhang, and zhongyuan wang. 2020.\ntable fact verification with structure-aware trans-\nformer. in proceedings of the 2020 conference on\nempirical methods in natural language processing\n(emnlp), pages 1624–1629, online. association for\ncomputational linguistics.\ntianshu zhang, xiang yue, yifei li, and huan sun.\n2023. tablellama: towards open large generalist\nmodels for tables.\nboyuan zheng, boyu gou, jihyung kil, huan sun, and\nyu su. 2024. gpt-4v (ision) is a generalist web agent,\nif grounded. arxiv preprint arxiv:2401.01614.\na contributions\nidea proposal. naihao deng proposed the idea\nof evaluating llms’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nbackground survey. zhenjie sun conducted a\ncareful survey on table-related tasks. naihao deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in westlake\nuniversity with yue zhang.\nimplementation. zhenjie sun came up with var-\nious table representations and designed all the\nprompts. zhenjie sun also implemented the eval-\nuation pipeline for autonomous metrics. naihao\ndeng and zhenjie sun implemented the pipeline\nfor human evaluation.\nexperiments. zhenjie sun and naihao deng\nconducted all the experiments. specifically,\nzhenjie sun conducted experiments on gpt-3.5,\ngpt-4 with text-based table representations, and\ngeminipro. naihao deng conducted experiments\non llama-2 models and gpt-4 with image-based\ntable representations.\nresult aggregation. zhenjie sun conducted the\nresult aggregation for all the experiments.\nhuman evaluation. zhenjie sun, naihao deng,\nruiqi he, aman sikka conducted the human evalu-\nation for the model generation.\npaper writing. naihao deng drafted the paper.\nzhenjie sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. naihao deng and zhenjie sun plot-\nted all the tables and figures. naihao deng, zhenjie\nsun selected examples that appeared in this paper.\nruiqi he, aman sikka provided suggestions on\nexample selections. rada mihalcea, yue zhang,"
    ]
  },
  {
    "question": "What is the abstract of the paper?",
    "answer": "The abstract of the paper titled \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" states that the study investigates the effectiveness of various large language models (LLMs) in interpreting tabular data through different prompting strategies and data formats. It includes analyses across six benchmarks for table-related tasks such as question-answering and fact-checking. The paper introduces an assessment of LLMs' performance on image-based table representations for the first time and compares the outcomes of five text-based and three image-based table formats. This work aims to provide insights into the effective use of LLMs on table-related tasks, emphasizing the challenges and nuances in processing structured data [1].",
    "references": [
      "a microsoft foundational model grant.\nreferences\njosh achiam, steven adler, sandhini agarwal, lama\nahmad, ilge akkaya, florencia leoni aleman,\ndiogo almeida, janko altenschmidt, sam altman,\nshyamal anadkat, et al. 2023. gpt-4 technical report.\narxiv preprint arxiv:2303.08774.\nmubashara akhtar, oana cocarascu, and elena simperl.\n2022. pubhealthtab: a public health table-based\ndataset for evidence-based fact checking. in find-\nings of the association for computational linguistics:\nnaacl 2022, pages 1–16, seattle, united states. as-\nsociation for computational linguistics.\nsyeda nahida akter, zichun yu, aashiq muhamed,\ntianyue ou, alex bäuerle, ángel alexander cabrera,\nkrish dholakia, chenyan xiong, and graham neu-\nbig. 2023. an in-depth look at gemini’s language\nabilities. arxiv preprint arxiv:2312.11444.\nhyojin bahng, ali jahanian, swami sankaranarayanan,\nand phillip isola. 2022. exploring visual prompts\nfor adapting large-scale models. arxiv preprint\narxiv:2203.17274.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, quyet v. do, yan xu,\nand pascale fung. 2023a. a multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. in proceedings of the\n13th international joint conference on natural lan-\nguage processing and the 3rd conference of the asia-\npacific chapter of the association for computational\nlinguistics (volume 1: long papers), pages 675–718,\nnusa dua, bali. association for computational lin-\nguistics.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, et al. 2023b. a multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arxiv\npreprint arxiv:2302.04023.\nemily m bender, timnit gebru, angelina mcmillan-\nmajor, and shmargaret shmitchell. 2021. on the\ndangers of stochastic parrots: can language models\nbe too big? in proceedings of the 2021 acm confer-\nence on fairness, accountability, and transparency,\npages 610–623.\ntom brown, benjamin mann, nick ryder, melanie\nsubbiah, jared d kaplan, prafulla dhariwal, arvind\nneelakantan, pranav shyam, girish sastry, amanda\naskell, et al. 2020. language models are few-shot\nlearners. advances in neural information processing\nsystems, 33:1877–1901.\nsébastien bubeck, varun chandrasekaran, ronen el-\ndan, johannes gehrke, eric horvitz, ece kamar,\npeter lee, yin tat lee, yuanzhi li, scott lund-\nberg, et al. 2023. sparks of artificial general intelli-\ngence: early experiments with gpt-4. arxiv preprint\narxiv:2303.12712.\nmiao chen, xinjiang lu, tong xu, yanyan li, zhou\njingbo, dejing dou, and hui xiong. 2022. to-\nwards table-to-text generation with pretrained lan-",
      "wonseok hwang, jinyeong yim, seunghyun park, and\nminjoon seo. 2019. a comprehensive exploration\non wikisql with table-aware word contextualization.\narxiv preprint arxiv:1902.01069.\nshima imani, liang du, and harsh shrivastava. 2023.\nmathprompter: mathematical reasoning using large\nlanguage models. in proceedings of the 61st an-\nnual meeting of the association for computational\nlinguistics (volume 5: industry track), pages 37–\n42, toronto, canada. association for computational\nlinguistics.\nzhijing jin, sydney levine, fernando gonzalez adauto,\nojasv kamal, maarten sap, mrinmaya sachan, rada\nmihalcea, josh tenenbaum, and bernhard schölkopf.\n2022. when to make exceptions: exploring language\nmodels as accounts of human moral judgment. in\nadvances in neural information processing systems,\nvolume 35, pages 28458–28473. curran associates,\ninc.\npengfei liu, weizhe yuan, jinlan fu, zhengbao jiang,\nhiroaki hayashi, and graham neubig. 2023. pre-\ntrain, prompt, and predict: a systematic survey of\nprompting methods in natural language processing.\nacm computing surveys, 55(9):1–35.\nqian liu, bei chen, jiaqi guo, morteza ziyadi, zeqi\nlin, weizhu chen, and jian-guang lou. 2021.\ntapex: table pre-training via learning a neural sql\nexecutor. arxiv preprint arxiv:2107.07653.\njekaterina novikova, ondˇ rej dušek, and verena rieser.\n2017. the e2e dataset: new challenges for end-\nto-end generation. in proceedings of the 18th an-\nnual sigdial meeting on discourse and dialogue,\npages 201–206, saarbrücken, germany. association\nfor computational linguistics.\nlong ouyang, jeffrey wu, xu jiang, diogo almeida,\ncarroll wainwright, pamela mishkin, chong zhang,\nsandhini agarwal, katarina slama, alex ray, et al.\n2022. training language models to follow instruc-\ntions with human feedback. advances in neural\ninformation processing systems, 35:27730–27744.\nankur parikh, xuezhi wang, sebastian gehrmann, man-\naal faruqui, bhuwan dhingra, diyi yang, and dipan-\njan das. 2020. totto: a controlled table-to-text\ngeneration dataset. in proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing (emnlp), pages 1173–1186, online. as-\nsociation for computational linguistics.\npanupong pasupat and percy liang. 2015. composi-\ntional semantic parsing on semi-structured tables. in\nproceedings of the 53rd annual meeting of the as-\nsociation for computational linguistics and the 7th\ninternational joint conference on natural language\nprocessing (volume 1: long papers), pages 1470–\n1480, beijing, china. association for computational\nlinguistics.\nshuofei qiao, yixin ou, ningyu zhang, xiang chen,\nyunzhi yao, shumin deng, chuanqi tan, fei huang,\nand huajun chen. 2022. reasoning with lan-\nguage model prompting: a survey. arxiv preprint\narxiv:2212.09597.\nalec radford, jong wook kim, chris hallacy, aditya\nramesh, gabriel goh, sandhini agarwal, girish sas-\ntry, amanda askell, pamela mishkin, jack clark,\net al. 2021. learning transferable visual models from\nnatural language supervision. in international confer-\nence on machine learning, pages 8748–8763. pmlr.\nbaptiste roziere, jonas gehring, fabian gloeckle, sten",
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect"
    ]
  },
  {
    "question": "What is the introduction of the paper?",
    "answer": "The introduction of the paper titled \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" discusses the rapid development of large language models (LLMs) and their impressive performance across various natural language processing tasks. However, it notes that there is less exploration regarding their effectiveness on structured data, like tables. Tables are emphasized as organized structures containing significant amounts of information, making them foundational for many applications, including medical diagnostics and customer relationship management.\n\nThe introduction elaborates on the challenges of evaluating LLMs' performance on tabular data due to the varying ways information can be represented in tables—whether in text or image form. The paper aims to systematically evaluate LLM performance using both textual and multimodal representations, addressing how different prompting strategies affect task performance, and providing insights for optimizing the processing of tabular data through LLMs [1].",
    "references": [
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect",
      "wonseok hwang, jinyeong yim, seunghyun park, and\nminjoon seo. 2019. a comprehensive exploration\non wikisql with table-aware word contextualization.\narxiv preprint arxiv:1902.01069.\nshima imani, liang du, and harsh shrivastava. 2023.\nmathprompter: mathematical reasoning using large\nlanguage models. in proceedings of the 61st an-\nnual meeting of the association for computational\nlinguistics (volume 5: industry track), pages 37–\n42, toronto, canada. association for computational\nlinguistics.\nzhijing jin, sydney levine, fernando gonzalez adauto,\nojasv kamal, maarten sap, mrinmaya sachan, rada\nmihalcea, josh tenenbaum, and bernhard schölkopf.\n2022. when to make exceptions: exploring language\nmodels as accounts of human moral judgment. in\nadvances in neural information processing systems,\nvolume 35, pages 28458–28473. curran associates,\ninc.\npengfei liu, weizhe yuan, jinlan fu, zhengbao jiang,\nhiroaki hayashi, and graham neubig. 2023. pre-\ntrain, prompt, and predict: a systematic survey of\nprompting methods in natural language processing.\nacm computing surveys, 55(9):1–35.\nqian liu, bei chen, jiaqi guo, morteza ziyadi, zeqi\nlin, weizhu chen, and jian-guang lou. 2021.\ntapex: table pre-training via learning a neural sql\nexecutor. arxiv preprint arxiv:2107.07653.\njekaterina novikova, ondˇ rej dušek, and verena rieser.\n2017. the e2e dataset: new challenges for end-\nto-end generation. in proceedings of the 18th an-\nnual sigdial meeting on discourse and dialogue,\npages 201–206, saarbrücken, germany. association\nfor computational linguistics.\nlong ouyang, jeffrey wu, xu jiang, diogo almeida,\ncarroll wainwright, pamela mishkin, chong zhang,\nsandhini agarwal, katarina slama, alex ray, et al.\n2022. training language models to follow instruc-\ntions with human feedback. advances in neural\ninformation processing systems, 35:27730–27744.\nankur parikh, xuezhi wang, sebastian gehrmann, man-\naal faruqui, bhuwan dhingra, diyi yang, and dipan-\njan das. 2020. totto: a controlled table-to-text\ngeneration dataset. in proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing (emnlp), pages 1173–1186, online. as-\nsociation for computational linguistics.\npanupong pasupat and percy liang. 2015. composi-\ntional semantic parsing on semi-structured tables. in\nproceedings of the 53rd annual meeting of the as-\nsociation for computational linguistics and the 7th\ninternational joint conference on natural language\nprocessing (volume 1: long papers), pages 1470–\n1480, beijing, china. association for computational\nlinguistics.\nshuofei qiao, yixin ou, ningyu zhang, xiang chen,\nyunzhi yao, shumin deng, chuanqi tan, fei huang,\nand huajun chen. 2022. reasoning with lan-\nguage model prompting: a survey. arxiv preprint\narxiv:2212.09597.\nalec radford, jong wook kim, chris hallacy, aditya\nramesh, gabriel goh, sandhini agarwal, girish sas-\ntry, amanda askell, pamela mishkin, jack clark,\net al. 2021. learning transferable visual models from\nnatural language supervision. in international confer-\nence on machine learning, pages 8748–8763. pmlr.\nbaptiste roziere, jonas gehring, fabian gloeckle, sten",
      "natural language and code for language agents. arxiv\npreprint arxiv:2310.06830.\njingfeng yang, aditya gupta, shyam upadhyay,\nluheng he, rahul goel, and shachi paul. 2022.\ntableformer: robust transformer modeling for table-\ntext encoding. in proceedings of the 60th annual\nmeeting of the association for computational lin-\nguistics (volume 1: long papers), pages 528–537,\ndublin, ireland. association for computational lin-\nguistics.\nzhengyuan yang, linjie li, kevin lin, jianfeng\nwang, chung-ching lin, zicheng liu, and lijuan\nwang. 2023. the dawn of lmms: preliminary\nexplorations with gpt-4v (ision). arxiv preprint\narxiv:2309.17421, 9(1):1.\npengcheng yin, graham neubig, wen-tau yih, and se-\nbastian riedel. 2020. tabert: pretraining for joint\nunderstanding of textual and tabular data. in proceed-\nings of the 58th annual meeting of the association\nfor computational linguistics, pages 8413–8426, on-\nline. association for computational linguistics.\nliangyu zha, junlin zhou, liyao li, rui wang, qingyi\nhuang, saisai yang, jing yuan, changbao su, xiang\nli, aofeng su, et al. 2023. tablegpt: towards unify-\ning tables, nature language and commands into one\ngpt. arxiv preprint arxiv:2307.08674.\nhongzhi zhang, yingyao wang, sirui wang, xuezhi\ncao, fuzheng zhang, and zhongyuan wang. 2020.\ntable fact verification with structure-aware trans-\nformer. in proceedings of the 2020 conference on\nempirical methods in natural language processing\n(emnlp), pages 1624–1629, online. association for\ncomputational linguistics.\ntianshu zhang, xiang yue, yifei li, and huan sun.\n2023. tablellama: towards open large generalist\nmodels for tables.\nboyuan zheng, boyu gou, jihyung kil, huan sun, and\nyu su. 2024. gpt-4v (ision) is a generalist web agent,\nif grounded. arxiv preprint arxiv:2401.01614.\na contributions\nidea proposal. naihao deng proposed the idea\nof evaluating llms’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nbackground survey. zhenjie sun conducted a\ncareful survey on table-related tasks. naihao deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in westlake\nuniversity with yue zhang.\nimplementation. zhenjie sun came up with var-\nious table representations and designed all the\nprompts. zhenjie sun also implemented the eval-\nuation pipeline for autonomous metrics. naihao\ndeng and zhenjie sun implemented the pipeline\nfor human evaluation.\nexperiments. zhenjie sun and naihao deng\nconducted all the experiments. specifically,\nzhenjie sun conducted experiments on gpt-3.5,\ngpt-4 with text-based table representations, and\ngeminipro. naihao deng conducted experiments\non llama-2 models and gpt-4 with image-based\ntable representations.\nresult aggregation. zhenjie sun conducted the\nresult aggregation for all the experiments.\nhuman evaluation. zhenjie sun, naihao deng,\nruiqi he, aman sikka conducted the human evalu-\nation for the model generation.\npaper writing. naihao deng drafted the paper.\nzhenjie sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. naihao deng and zhenjie sun plot-\nted all the tables and figures. naihao deng, zhenjie\nsun selected examples that appeared in this paper.\nruiqi he, aman sikka provided suggestions on\nexample selections. rada mihalcea, yue zhang,"
    ]
  },
  {
    "question": "What is the methodology of the paper?",
    "answer": "The methodology of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" involves systematically evaluating the performance of various large language models (LLMs) on interpreting tabular data through different prompting strategies and representations. The study employs multiple benchmarks specifically designed for table-related tasks, including question-answering, fact-checking, and generation tasks involving tables.\n\nKey components of the methodology include:\n\n1. **Model Evaluation**: The models being studied include text-based and multimodal LLMs, specifically assessing their capabilities with both text representations and image representations of tables.\n   \n2. **Prompting Strategies**: Various prompting strategies are applied to both text- and image-based representations to understand their influence on model performance. The study tests several prompting techniques like vanilla, expert, and chain-of-thought prompting to identify which method yields the best results.\n   \n3. **Statistical Analysis**: The methodology includes rigorous statistical testing, such as Fisher’s exact test, McNemar’s test, and proportion z tests, to determine the significance of differences in model performance across various tasks and representations.\n\n4. **Performance Benchmarks**: The authors design and utilize a set of benchmarks to evaluate model performance quantitatively. They present averaged accuracy scores and ROUGE-L scores for different datasets, providing a comprehensive overview of how well LLMs can interpret and generate information based on tabular data.\n\n5. **Human Evaluation**: There is also a qualitative aspect where human evaluators assess model outputs to ensure that the generated sentences capture all the relevant information from the tables accurately.\n\nThrough this methodology, the paper aims to provide insights into optimizing LLM use on tasks that involve structured data in tabular format [1].",
    "references": [
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect",
      "wonseok hwang, jinyeong yim, seunghyun park, and\nminjoon seo. 2019. a comprehensive exploration\non wikisql with table-aware word contextualization.\narxiv preprint arxiv:1902.01069.\nshima imani, liang du, and harsh shrivastava. 2023.\nmathprompter: mathematical reasoning using large\nlanguage models. in proceedings of the 61st an-\nnual meeting of the association for computational\nlinguistics (volume 5: industry track), pages 37–\n42, toronto, canada. association for computational\nlinguistics.\nzhijing jin, sydney levine, fernando gonzalez adauto,\nojasv kamal, maarten sap, mrinmaya sachan, rada\nmihalcea, josh tenenbaum, and bernhard schölkopf.\n2022. when to make exceptions: exploring language\nmodels as accounts of human moral judgment. in\nadvances in neural information processing systems,\nvolume 35, pages 28458–28473. curran associates,\ninc.\npengfei liu, weizhe yuan, jinlan fu, zhengbao jiang,\nhiroaki hayashi, and graham neubig. 2023. pre-\ntrain, prompt, and predict: a systematic survey of\nprompting methods in natural language processing.\nacm computing surveys, 55(9):1–35.\nqian liu, bei chen, jiaqi guo, morteza ziyadi, zeqi\nlin, weizhu chen, and jian-guang lou. 2021.\ntapex: table pre-training via learning a neural sql\nexecutor. arxiv preprint arxiv:2107.07653.\njekaterina novikova, ondˇ rej dušek, and verena rieser.\n2017. the e2e dataset: new challenges for end-\nto-end generation. in proceedings of the 18th an-\nnual sigdial meeting on discourse and dialogue,\npages 201–206, saarbrücken, germany. association\nfor computational linguistics.\nlong ouyang, jeffrey wu, xu jiang, diogo almeida,\ncarroll wainwright, pamela mishkin, chong zhang,\nsandhini agarwal, katarina slama, alex ray, et al.\n2022. training language models to follow instruc-\ntions with human feedback. advances in neural\ninformation processing systems, 35:27730–27744.\nankur parikh, xuezhi wang, sebastian gehrmann, man-\naal faruqui, bhuwan dhingra, diyi yang, and dipan-\njan das. 2020. totto: a controlled table-to-text\ngeneration dataset. in proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing (emnlp), pages 1173–1186, online. as-\nsociation for computational linguistics.\npanupong pasupat and percy liang. 2015. composi-\ntional semantic parsing on semi-structured tables. in\nproceedings of the 53rd annual meeting of the as-\nsociation for computational linguistics and the 7th\ninternational joint conference on natural language\nprocessing (volume 1: long papers), pages 1470–\n1480, beijing, china. association for computational\nlinguistics.\nshuofei qiao, yixin ou, ningyu zhang, xiang chen,\nyunzhi yao, shumin deng, chuanqi tan, fei huang,\nand huajun chen. 2022. reasoning with lan-\nguage model prompting: a survey. arxiv preprint\narxiv:2212.09597.\nalec radford, jong wook kim, chris hallacy, aditya\nramesh, gabriel goh, sandhini agarwal, girish sas-\ntry, amanda askell, pamela mishkin, jack clark,\net al. 2021. learning transferable visual models from\nnatural language supervision. in international confer-\nence on machine learning, pages 8748–8763. pmlr.\nbaptiste roziere, jonas gehring, fabian gloeckle, sten",
      "guage model: a table structure understanding and\ntext deliberating approach. in proceedings of the\n2022 conference on empirical methods in natu-\nral language processing, pages 8199–8210, abu\ndhabi, united arab emirates. association for com-\nputational linguistics.\nsanxing chen, xiaodong liu, jianfeng gao, jian jiao,\nruofei zhang, and yangfeng ji. 2021a. hitter: hi-\nerarchical transformers for knowledge graph embed-\ndings. in proceedings of the 2021 conference on\nempirical methods in natural language process-\ning, pages 10395–10407, online and punta cana,\ndominican republic. association for computational\nlinguistics.\nwenhu chen, jianshu chen, yu su, zhiyu chen, and\nwilliam yang wang. 2020. logical natural language\ngeneration from open-domain tables. in proceedings\nof the 58th annual meeting of the association for\ncomputational linguistics, pages 7929–7942, online.\nassociation for computational linguistics.\nwenhu chen, hongmin wang, jianshu chen, yunkai\nzhang, hong wang, shiyang li, xiyou zhou, and\nwilliam yang wang. 2019. tabfact: a large-\nscale dataset for table-based fact verification. arxiv\npreprint arxiv:1909.02164.\nzhiyu chen, wenhu chen, charese smiley, sameena\nshah, iana borova, dylan langdon, reema moussa,\nmatt beane, ting-hao huang, bryan routledge, and\nwilliam yang wang. 2021b. finqa: a dataset of nu-\nmerical reasoning over financial data. in proceedings\nof the 2021 conference on empirical methods in nat-\nural language processing, pages 3697–3711, online\nand punta cana, dominican republic. association\nfor computational linguistics.\ntianji cong, madelon hulsebos, zhenjie sun, paul\ngroth, and h. v. jagadish. 2024. observatory: char-\nacterizing embeddings of relational tables.\ndeborah a. dahl, madeleine bates, michael brown,\nwilliam fisher, kate hunicke-smith, david pallett,\nchristine pao, alexander rudnicky, and elizabeth\nshriberg. 1994. expanding the scope of the atis\ntask: the atis-3 corpus. in human language tech-\nnology: proceedings of a workshop held at plains-\nboro, new jersey, march 8-11, 1994.\nnaihao deng, yulong chen, and yue zhang. 2022. re-\ncent advances in text-to-sql: a survey of what we\nhave and what we expect. in proceedings of the\n29th international conference on computational lin-\nguistics, pages 2166–2187, gyeongju, republic of\nkorea. international committee on computational\nlinguistics.\nxiang deng, huan sun, alyssa lees, you wu, and\ncong yu. 2020. turl: table understanding through\nrepresentation learning. proceedings of the vldb\nendowment, 14(3):307–319.\ncharles t. hemphill, john j. godfrey, and george r.\ndoddington. 1990. the atis spoken language sys-\ntems pilot corpus. in speech and natural language:\nproceedings of a workshop held at hidden valley,\npennsylvania, june 24-27,1990.\ndan hendrycks, collin burns, saurav kadavath, akul\narora, steven basart, eric tang, dawn song, and\njacob steinhardt. 2021. measuring mathematical\nproblem solving with the math dataset. neurips.\njonathan herzig, pawel krzysztof nowak, thomas\nmüller, francesco piccinno, and julian eisenschlos.\n2020. tapas: weakly supervised table parsing via\npre-training. in proceedings of the 58th annual meet-\ning of the association for computational linguistics,\npages 4320–4333, online. association for computa-\ntional linguistics."
    ]
  },
  {
    "question": "What is the related work of the paper?",
    "answer": "The related work section of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" provides an overview of prior research in the field of table processing and understanding within natural language processing (NLP). This section highlights various studies that have explored the capabilities of language models in handling structured data like tables, focusing on their limitations and the tasks they were designed to address.\n\nKey aspects covered in the related work include:\n\n1. **Table Representations**: Previous research has examined numerous ways to represent tabular information for LLMs, including text-based representations and innovative approaches involving image processing. These methodologies are critical for understanding how best to present tabular data to improve model performance.\n\n2. **Evaluation Datasets**: The paper references existing benchmarks that have been used to assess model performance on table-related tasks, including datasets for question-answering, fact-checking, and numerical reasoning over financial data. The effectiveness of these datasets in evaluating LLMs is a key point of interest.\n\n3. **Multimodal Learning**: There has been growing interest in multimodal models that can process both visual and textual information. Previous studies have shown that models like GPT-4 and Gemini exhibit improved performance when trained on or exposed to multiple formats simultaneously.\n\n4. **Challenges and Limitations**: Challenges such as arithmetic reasoning and spatial understanding when working with tabular data have been documented in earlier works. These limitations underscore the necessity for further research to enhance LLM performance in these areas.\n\nThe related work section serves to contextualize the current study within the broader landscape of ongoing research, identifying gaps that the current work aims to fill and establishing the significance of the proposed methodologies and evaluations [1].",
    "references": [
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect",
      "a microsoft foundational model grant.\nreferences\njosh achiam, steven adler, sandhini agarwal, lama\nahmad, ilge akkaya, florencia leoni aleman,\ndiogo almeida, janko altenschmidt, sam altman,\nshyamal anadkat, et al. 2023. gpt-4 technical report.\narxiv preprint arxiv:2303.08774.\nmubashara akhtar, oana cocarascu, and elena simperl.\n2022. pubhealthtab: a public health table-based\ndataset for evidence-based fact checking. in find-\nings of the association for computational linguistics:\nnaacl 2022, pages 1–16, seattle, united states. as-\nsociation for computational linguistics.\nsyeda nahida akter, zichun yu, aashiq muhamed,\ntianyue ou, alex bäuerle, ángel alexander cabrera,\nkrish dholakia, chenyan xiong, and graham neu-\nbig. 2023. an in-depth look at gemini’s language\nabilities. arxiv preprint arxiv:2312.11444.\nhyojin bahng, ali jahanian, swami sankaranarayanan,\nand phillip isola. 2022. exploring visual prompts\nfor adapting large-scale models. arxiv preprint\narxiv:2203.17274.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, quyet v. do, yan xu,\nand pascale fung. 2023a. a multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. in proceedings of the\n13th international joint conference on natural lan-\nguage processing and the 3rd conference of the asia-\npacific chapter of the association for computational\nlinguistics (volume 1: long papers), pages 675–718,\nnusa dua, bali. association for computational lin-\nguistics.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, et al. 2023b. a multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arxiv\npreprint arxiv:2302.04023.\nemily m bender, timnit gebru, angelina mcmillan-\nmajor, and shmargaret shmitchell. 2021. on the\ndangers of stochastic parrots: can language models\nbe too big? in proceedings of the 2021 acm confer-\nence on fairness, accountability, and transparency,\npages 610–623.\ntom brown, benjamin mann, nick ryder, melanie\nsubbiah, jared d kaplan, prafulla dhariwal, arvind\nneelakantan, pranav shyam, girish sastry, amanda\naskell, et al. 2020. language models are few-shot\nlearners. advances in neural information processing\nsystems, 33:1877–1901.\nsébastien bubeck, varun chandrasekaran, ronen el-\ndan, johannes gehrke, eric horvitz, ece kamar,\npeter lee, yin tat lee, yuanzhi li, scott lund-\nberg, et al. 2023. sparks of artificial general intelli-\ngence: early experiments with gpt-4. arxiv preprint\narxiv:2303.12712.\nmiao chen, xinjiang lu, tong xu, yanyan li, zhou\njingbo, dejing dou, and hui xiong. 2022. to-\nwards table-to-text generation with pretrained lan-",
      "indicates how we highlight the relevant cells, where\n“n/a” indicates no information about relevant cells, “t”\nindicates referring to relevant cells in the text, “v” in-\ndicates highlighting relevant cells on the table image,\n“v + t” indicates both highlighting relevant cells on the\ntable image and referring to them in the text.\nrq4. does highlighting relevant cells yield a\nbetter performance?\ntest: we test all six llms in section 3.1 on\nfinqa which provides relevant cells in the table\nfor each instance. we refer to the relevant cells\nby adding “please pay attention to the highlighted\ncells: (row index, column index, cell value)” in the\ntext prompt, or mark them on the table image di-\nrectly. appendix e provides our prompt examples.\nwe use vanilla prompting in this comparison.\ntl;dr answer: yes.\nfull answer: in table 6, we notice that in most\ncases, referring llms to specific cells helps llms\nbetter attend to them, thereby helping llms reason\nover the example. however, llms’ performance\nmay get hurt when we refer to the relevant cells\nthrough text such as llama-2-13b and 70b. this\nmay be due to the inherent limitations of textual\ndescriptions for conveying spatial or relational in-\nformation. in order to relate the mentioned cells in\nthe text, the model needs to figure out the connec-\ntion between the mentioned cell and the cell in the\nlinearized table, which can be challenging to the\nmodel given the complicated table structure.\nin addition, llms best attend to the table items\nwhen there are clues from both text and image. in\ntable 6, we observe that marking the relevant cells\non the image while mentioning them through text\nleads to the most correctly answered examples (61\nexamples by gpt-4 and 38 by geminipro at the last\nrow in table 6). such a dual-modality approach\nthat combines visual cues with text references, en-\n∆ metric\nfinqa 47.0 57.0 +10.0\nlogicnlg 43.4 58.5 +15.1\ntabfact 51.8 74.7 +22.9\nwikitq 69.0 86.0 +17.0\nacc\ne2e 37.1 46.0 + 8.9 rouge-l\ntotto 30.1 47.7 +17.6\ntable 7: performance scores of the best performed\nopen-source ( ) llm we test, llama-2-70b versus\nclosed-source ( ) llm we test, gpt-4 on different\ndatasets. the closed-source llms always outperform\nthe open-source llms and we report the performance\ndifference ∆ between them. for consistency across dif-\nferent datasets, we do not include the performances\nwith highlighting cells*in this comparison. table 20 in\nappendix d provides what combinations of table repre-\nsentation and prompting method yield this performance.\ndate country placing …\n10/31/2008 uk 1st …\n…\nquestion: what is the number of 1st\nplace finishes across all events?\ngold: 17\ngpt-4: 15 ⨯\nfigure 7: an example from wikitq where gpt-4 can-\nnot answer it correctly with bracket table representation\nand chain-of-thought prompting. in addition, for most\nof the table representations and prompting styles, gpt-4\ncannot answer this question correctly.\nhances llms’ overall reasoning ability over the\ntabular data.\nrq5 and rq6\nwe include two additional research questions and\nour answer to them in appendix c, including\nwhether these llms can reconstruct tables from\nthe image, and whether multimodal llms benefit\nfrom having both formats simultaneously as input.\n5 open problems to increase the\nperformance of llms on tabular data\nmathematical reasoning. we observe that llms\nare not good at arithmetic reasoning similar to the\nfindings in prior works (hendrycks et al., 2021;\nimani et al., 2023). as shown in figure 7, sim-\nple arithmetic computing like counting the total\nnumber of rows that satisfy certain conditions (‘1st’\nin figure 7) still poses challenges even for gpt-4.\n*except for totto, where the task is to generate the sen-\ntence based on the highlighted cells. on totto, we include\nthe highlight information just in text.\nquestion: write a sentence to describe the table.\nname eattype area\nblue spice pub riverside\nmodel generated\nthe table contains three columns and three rows, with\nl-7b\nl-13b\nthe data “blue spice”, “pub”, and “riverside” in the\nfirst column, respectively.\n⨯"
    ]
  },
  {
    "question": "What is the conclusion of the paper?",
    "answer": "The conclusion of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" is that large language models (LLMs) demonstrate reasonably effective performance when engaging with tabular data, whether represented textually or as images. The authors highlight the significant role that different prompting strategies and representations play in influencing model performance.\n\nKey takeaways from the conclusion include:\n\n1. **Effectiveness of Representations**: The study provides evidence that both text and image representations can yield decent performance from LLMs, with some image-based representations resulting in better outcomes under certain conditions.\n\n2. **Prompting Nuances**: The research emphasizes the importance of carefully designed prompts for optimal performance on table-related tasks, highlighting that the choice of prompting method can significantly affect the results achieved by the models.\n\n3. **New Insights**: The paper contributes new insights into the processing of tabular data by LLMs, underlining the complexities involved and the need for ongoing research to further optimize their capabilities in handling structured data. \n\nOverall, the findings encourage the advancement of methodologies to better bridge the performance gap in understanding and utilizing tabular data across various applications [1].",
    "references": [
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect",
      "a microsoft foundational model grant.\nreferences\njosh achiam, steven adler, sandhini agarwal, lama\nahmad, ilge akkaya, florencia leoni aleman,\ndiogo almeida, janko altenschmidt, sam altman,\nshyamal anadkat, et al. 2023. gpt-4 technical report.\narxiv preprint arxiv:2303.08774.\nmubashara akhtar, oana cocarascu, and elena simperl.\n2022. pubhealthtab: a public health table-based\ndataset for evidence-based fact checking. in find-\nings of the association for computational linguistics:\nnaacl 2022, pages 1–16, seattle, united states. as-\nsociation for computational linguistics.\nsyeda nahida akter, zichun yu, aashiq muhamed,\ntianyue ou, alex bäuerle, ángel alexander cabrera,\nkrish dholakia, chenyan xiong, and graham neu-\nbig. 2023. an in-depth look at gemini’s language\nabilities. arxiv preprint arxiv:2312.11444.\nhyojin bahng, ali jahanian, swami sankaranarayanan,\nand phillip isola. 2022. exploring visual prompts\nfor adapting large-scale models. arxiv preprint\narxiv:2203.17274.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, quyet v. do, yan xu,\nand pascale fung. 2023a. a multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. in proceedings of the\n13th international joint conference on natural lan-\nguage processing and the 3rd conference of the asia-\npacific chapter of the association for computational\nlinguistics (volume 1: long papers), pages 675–718,\nnusa dua, bali. association for computational lin-\nguistics.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, et al. 2023b. a multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arxiv\npreprint arxiv:2302.04023.\nemily m bender, timnit gebru, angelina mcmillan-\nmajor, and shmargaret shmitchell. 2021. on the\ndangers of stochastic parrots: can language models\nbe too big? in proceedings of the 2021 acm confer-\nence on fairness, accountability, and transparency,\npages 610–623.\ntom brown, benjamin mann, nick ryder, melanie\nsubbiah, jared d kaplan, prafulla dhariwal, arvind\nneelakantan, pranav shyam, girish sastry, amanda\naskell, et al. 2020. language models are few-shot\nlearners. advances in neural information processing\nsystems, 33:1877–1901.\nsébastien bubeck, varun chandrasekaran, ronen el-\ndan, johannes gehrke, eric horvitz, ece kamar,\npeter lee, yin tat lee, yuanzhi li, scott lund-\nberg, et al. 2023. sparks of artificial general intelli-\ngence: early experiments with gpt-4. arxiv preprint\narxiv:2303.12712.\nmiao chen, xinjiang lu, tong xu, yanyan li, zhou\njingbo, dejing dou, and hui xiong. 2022. to-\nwards table-to-text generation with pretrained lan-",
      "wonseok hwang, jinyeong yim, seunghyun park, and\nminjoon seo. 2019. a comprehensive exploration\non wikisql with table-aware word contextualization.\narxiv preprint arxiv:1902.01069.\nshima imani, liang du, and harsh shrivastava. 2023.\nmathprompter: mathematical reasoning using large\nlanguage models. in proceedings of the 61st an-\nnual meeting of the association for computational\nlinguistics (volume 5: industry track), pages 37–\n42, toronto, canada. association for computational\nlinguistics.\nzhijing jin, sydney levine, fernando gonzalez adauto,\nojasv kamal, maarten sap, mrinmaya sachan, rada\nmihalcea, josh tenenbaum, and bernhard schölkopf.\n2022. when to make exceptions: exploring language\nmodels as accounts of human moral judgment. in\nadvances in neural information processing systems,\nvolume 35, pages 28458–28473. curran associates,\ninc.\npengfei liu, weizhe yuan, jinlan fu, zhengbao jiang,\nhiroaki hayashi, and graham neubig. 2023. pre-\ntrain, prompt, and predict: a systematic survey of\nprompting methods in natural language processing.\nacm computing surveys, 55(9):1–35.\nqian liu, bei chen, jiaqi guo, morteza ziyadi, zeqi\nlin, weizhu chen, and jian-guang lou. 2021.\ntapex: table pre-training via learning a neural sql\nexecutor. arxiv preprint arxiv:2107.07653.\njekaterina novikova, ondˇ rej dušek, and verena rieser.\n2017. the e2e dataset: new challenges for end-\nto-end generation. in proceedings of the 18th an-\nnual sigdial meeting on discourse and dialogue,\npages 201–206, saarbrücken, germany. association\nfor computational linguistics.\nlong ouyang, jeffrey wu, xu jiang, diogo almeida,\ncarroll wainwright, pamela mishkin, chong zhang,\nsandhini agarwal, katarina slama, alex ray, et al.\n2022. training language models to follow instruc-\ntions with human feedback. advances in neural\ninformation processing systems, 35:27730–27744.\nankur parikh, xuezhi wang, sebastian gehrmann, man-\naal faruqui, bhuwan dhingra, diyi yang, and dipan-\njan das. 2020. totto: a controlled table-to-text\ngeneration dataset. in proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing (emnlp), pages 1173–1186, online. as-\nsociation for computational linguistics.\npanupong pasupat and percy liang. 2015. composi-\ntional semantic parsing on semi-structured tables. in\nproceedings of the 53rd annual meeting of the as-\nsociation for computational linguistics and the 7th\ninternational joint conference on natural language\nprocessing (volume 1: long papers), pages 1470–\n1480, beijing, china. association for computational\nlinguistics.\nshuofei qiao, yixin ou, ningyu zhang, xiang chen,\nyunzhi yao, shumin deng, chuanqi tan, fei huang,\nand huajun chen. 2022. reasoning with lan-\nguage model prompting: a survey. arxiv preprint\narxiv:2212.09597.\nalec radford, jong wook kim, chris hallacy, aditya\nramesh, gabriel goh, sandhini agarwal, girish sas-\ntry, amanda askell, pamela mishkin, jack clark,\net al. 2021. learning transferable visual models from\nnatural language supervision. in international confer-\nence on machine learning, pages 8748–8763. pmlr.\nbaptiste roziere, jonas gehring, fabian gloeckle, sten"
    ]
  },
  {
    "question": "What is the future work of the paper?",
    "answer": "The future work section of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" suggests several avenues for further exploration to enhance understanding and processing of tabular data by large language models (LLMs). Key areas identified for future research include:\n\n1. **Improving Model Architectures**: There is a call for the development of more advanced model architectures that can better understand and process the complexities of table structures and relationships within data.\n\n2. **Enhanced Evaluation Metrics**: The authors propose creating new metrics and benchmarks specifically designed to accurately assess model performance on tabular data, going beyond traditional metrics to encompass aspects like reasoning ability and spatial relations in tables.\n\n3. **Multimodal Integration**: Future research should investigate ways to optimize the integration of multimodal input (text and images) to enhance LLM performance on table-related tasks. This may involve refining how visual cues in image representations can be explicitly utilized alongside textual information.\n\n4. **Expanded Datasets**: The creation and use of larger and more diverse datasets that capture a wider range of table structures and contexts could help improve model training and evaluation, fostering better generalization across different applications.\n\n5. **Exploration of Specific Applications**: There is potential for exploring specific applications of LLMs in areas such as healthcare and customer service, where processing tabular data is crucial.\n\nBy addressing these research directions, the authors believe that future work can significantly improve how LLMs interact with tabular data, ultimately enhancing their reasoning and interpretative capabilities [1].",
    "references": [
      "a microsoft foundational model grant.\nreferences\njosh achiam, steven adler, sandhini agarwal, lama\nahmad, ilge akkaya, florencia leoni aleman,\ndiogo almeida, janko altenschmidt, sam altman,\nshyamal anadkat, et al. 2023. gpt-4 technical report.\narxiv preprint arxiv:2303.08774.\nmubashara akhtar, oana cocarascu, and elena simperl.\n2022. pubhealthtab: a public health table-based\ndataset for evidence-based fact checking. in find-\nings of the association for computational linguistics:\nnaacl 2022, pages 1–16, seattle, united states. as-\nsociation for computational linguistics.\nsyeda nahida akter, zichun yu, aashiq muhamed,\ntianyue ou, alex bäuerle, ángel alexander cabrera,\nkrish dholakia, chenyan xiong, and graham neu-\nbig. 2023. an in-depth look at gemini’s language\nabilities. arxiv preprint arxiv:2312.11444.\nhyojin bahng, ali jahanian, swami sankaranarayanan,\nand phillip isola. 2022. exploring visual prompts\nfor adapting large-scale models. arxiv preprint\narxiv:2203.17274.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, quyet v. do, yan xu,\nand pascale fung. 2023a. a multitask, multilingual,\nmultimodal evaluation of chatgpt on reasoning, hal-\nlucination, and interactivity. in proceedings of the\n13th international joint conference on natural lan-\nguage processing and the 3rd conference of the asia-\npacific chapter of the association for computational\nlinguistics (volume 1: long papers), pages 675–718,\nnusa dua, bali. association for computational lin-\nguistics.\nyejin bang, samuel cahyawijaya, nayeon lee, wen-\nliang dai, dan su, bryan wilie, holy lovenia, ziwei\nji, tiezheng yu, willy chung, et al. 2023b. a multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arxiv\npreprint arxiv:2302.04023.\nemily m bender, timnit gebru, angelina mcmillan-\nmajor, and shmargaret shmitchell. 2021. on the\ndangers of stochastic parrots: can language models\nbe too big? in proceedings of the 2021 acm confer-\nence on fairness, accountability, and transparency,\npages 610–623.\ntom brown, benjamin mann, nick ryder, melanie\nsubbiah, jared d kaplan, prafulla dhariwal, arvind\nneelakantan, pranav shyam, girish sastry, amanda\naskell, et al. 2020. language models are few-shot\nlearners. advances in neural information processing\nsystems, 33:1877–1901.\nsébastien bubeck, varun chandrasekaran, ronen el-\ndan, johannes gehrke, eric horvitz, ece kamar,\npeter lee, yin tat lee, yuanzhi li, scott lund-\nberg, et al. 2023. sparks of artificial general intelli-\ngence: early experiments with gpt-4. arxiv preprint\narxiv:2303.12712.\nmiao chen, xinjiang lu, tong xu, yanyan li, zhou\njingbo, dejing dou, and hui xiong. 2022. to-\nwards table-to-text generation with pretrained lan-",
      "wonseok hwang, jinyeong yim, seunghyun park, and\nminjoon seo. 2019. a comprehensive exploration\non wikisql with table-aware word contextualization.\narxiv preprint arxiv:1902.01069.\nshima imani, liang du, and harsh shrivastava. 2023.\nmathprompter: mathematical reasoning using large\nlanguage models. in proceedings of the 61st an-\nnual meeting of the association for computational\nlinguistics (volume 5: industry track), pages 37–\n42, toronto, canada. association for computational\nlinguistics.\nzhijing jin, sydney levine, fernando gonzalez adauto,\nojasv kamal, maarten sap, mrinmaya sachan, rada\nmihalcea, josh tenenbaum, and bernhard schölkopf.\n2022. when to make exceptions: exploring language\nmodels as accounts of human moral judgment. in\nadvances in neural information processing systems,\nvolume 35, pages 28458–28473. curran associates,\ninc.\npengfei liu, weizhe yuan, jinlan fu, zhengbao jiang,\nhiroaki hayashi, and graham neubig. 2023. pre-\ntrain, prompt, and predict: a systematic survey of\nprompting methods in natural language processing.\nacm computing surveys, 55(9):1–35.\nqian liu, bei chen, jiaqi guo, morteza ziyadi, zeqi\nlin, weizhu chen, and jian-guang lou. 2021.\ntapex: table pre-training via learning a neural sql\nexecutor. arxiv preprint arxiv:2107.07653.\njekaterina novikova, ondˇ rej dušek, and verena rieser.\n2017. the e2e dataset: new challenges for end-\nto-end generation. in proceedings of the 18th an-\nnual sigdial meeting on discourse and dialogue,\npages 201–206, saarbrücken, germany. association\nfor computational linguistics.\nlong ouyang, jeffrey wu, xu jiang, diogo almeida,\ncarroll wainwright, pamela mishkin, chong zhang,\nsandhini agarwal, katarina slama, alex ray, et al.\n2022. training language models to follow instruc-\ntions with human feedback. advances in neural\ninformation processing systems, 35:27730–27744.\nankur parikh, xuezhi wang, sebastian gehrmann, man-\naal faruqui, bhuwan dhingra, diyi yang, and dipan-\njan das. 2020. totto: a controlled table-to-text\ngeneration dataset. in proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing (emnlp), pages 1173–1186, online. as-\nsociation for computational linguistics.\npanupong pasupat and percy liang. 2015. composi-\ntional semantic parsing on semi-structured tables. in\nproceedings of the 53rd annual meeting of the as-\nsociation for computational linguistics and the 7th\ninternational joint conference on natural language\nprocessing (volume 1: long papers), pages 1470–\n1480, beijing, china. association for computational\nlinguistics.\nshuofei qiao, yixin ou, ningyu zhang, xiang chen,\nyunzhi yao, shumin deng, chuanqi tan, fei huang,\nand huajun chen. 2022. reasoning with lan-\nguage model prompting: a survey. arxiv preprint\narxiv:2212.09597.\nalec radford, jong wook kim, chris hallacy, aditya\nramesh, gabriel goh, sandhini agarwal, girish sas-\ntry, amanda askell, pamela mishkin, jack clark,\net al. 2021. learning transferable visual models from\nnatural language supervision. in international confer-\nence on machine learning, pages 8748–8763. pmlr.\nbaptiste roziere, jonas gehring, fabian gloeckle, sten",
      "lin ma, and yulong chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nb statistical significance test results\nwe have observed significant differences between\nllms’ performance. we conduct three statistical\nsignificance tests, including fisher’s exact test, mc-\nnemar’s test, and proportion z test for the model\npredictions.\nwith p < 0.05:\n1. for fisher’s exact test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, logic-\nnlg, tabfact, wikitq (figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (table 5).\n2. for mcnemar’s test, we find statistically sig-\nnificant differences between gpt-4 perfor-\nmance between t and v inputs for finqa,\ntabfact, and wikitq (figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (table 4).\n3. for the proportion z test, we find a statistically\nsignificant difference in gpt-4 performance\nbetween t and v inputs for finqa, wikitq\n(figure 3).\nc research questions cont’d\nc.1 rq1 cont’d. can we use image-based\nrepresentations of tabular data?\nfigure 9 and figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to gpt-4 and\ngeminipro for chain-of-thought and expert prompt-\ning, respectively. the results resemble similar\ntrends as figure 3.\nc.2 rq2 cont’d. how do different text-based\nprompt methods affect llms’\nperformance on tabular-related tasks?\ntable 8 reports the averaged accuracy scores across\nfinqa, logicnlg, tabfact and wikitq that use\naccuracy as the metric. table 9 and table 11 re-\nport the rouge-l scores of llms’ generation on\ne2e and totto dataset, respectively. table 10 re-\nports the scores annotated manually by the authors.\nas discussed in section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. we assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\ngpt gmnpro\nllama-2\n3.5 4 7b 13b 70b\nvanilla-t\nv 52.5 e 51.0 cot 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nbracket\nv 50.9 e 47.9 cot 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\ncolumn-json\nv 48.3 e 48.8 cot 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nrow-json\nv 49.7 e 53.7 cot 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nrow-identifier\nv 52.0 e 53.2 cot 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\ntable 8: for text-based table representations, averaged\naccuracy scores across finqa, logicnlg, tabfact,\nand wikitq for different llms. “gmnpro” represents\ngeminipro model, “v”, “e”, and “cot” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nc.3 rq3 cont’d. how do different\nimage-based prompt methods affect"
    ]
  }
]
