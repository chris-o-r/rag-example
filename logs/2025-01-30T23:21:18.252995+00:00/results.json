[
  {
    "question": "What are the main objectives of the study on Table Information Seeking (TIS) in Large Language Models (LLMs)?",
    "answer": "The main objectives of the study on Table Information Seeking (TIS) in Large Language Models (LLMs) are to investigate the effectiveness of various LLMs in interpreting tabular data through different prompting strategies and data formats. The study evaluates model performance on tabular tasks such as question-answering and fact-checking using both text-based and image-based representations. It aims to understand how representation and prompting affect LLM performance, and it introduces the assessment of LLMs' performance on image-based table representations for the first time. The research questions include the effectiveness of these representations and the nuances of prompting methods for table-related tasks [1].",
    "references": [
      "arXiv:2402.12424v5 [cs.LG] 17 Oct 2024\nTables as Texts or Images:\nEvaluating the Table Reasoning Ability of LLMs and MLLMs\nNaihao Deng* , Zhenjie Sun* , Ruiqi He , Aman Sikka ,\nYulong Chen , Lin Ma , Yue Zhang , Rada Mihalcea\nUniversity of Michigan University of Cambridge Westlake University\n{dnaihao, zjsun}@umich.edu\nAbstract\nIn this paper, we investigate the effectiveness\nof various LLMs in interpreting tabular data\nthrough different prompting strategies and data\nformats. Our analyses extend across six bench-\nmarks for table-related tasks such as question-\nanswering and fact-checking. We introduce\nfor the first time the assessment of LLMs’\nperformance on image-based table represen-\ntations. Specifically, we compare five text-\nbased and three image-based table represen-\ntations, demonstrating the role of representa-\ntion and prompting on LLM performance. Our\nstudy provides insights into the effective use\nof LLMs on table-related tasks. Our data is\navailable at: https://github.com/dnaihao/\nTables-as-Texts-or-Images.\n1 Introduction\nRecent years have witnessed an explosion of Large\nLanguage Models (LLMs), with impressive per-\nformance on various Natural Language Processing\n(NLP) tasks (Brown et al., 2020; Touvron et al.,\n2023; Team et al., 2023). Research to date has\nexamined the performance of LLMs for various\naspects and abilities (Bang et al., 2023b; Bubeck\net al., 2023; Akter et al., 2023), but their effec-\ntiveness on structured data such as tables is less\nexplored.\nUnlike unstructured text, tables are systemat-\nically organized structures of a large amount of\ninformation. This characteristic makes tabular\ndata serve as the foundations for numerous ap-\nplications, including medical diagnostics, virtual\npersonal assistants, customer relationship manage-\nment (Hemphill et al., 1990; Dahl et al., 1994;\nAkhtar et al., 2022; Xie et al., 2022), etc.\nThe evaluation of LLMs on processing tabular\ndata involves many challenges. First, there are\nmany ways to represent the information in tables.\n*Contributed equally to this work. Appendix A lists the\ndetailed contributions.\nIf we represent the table in pure text, we may use\nnaive linearization or insert brackets to better rep-\nresent table structures. Meanwhile, emerging mul-\ntimodal LLMs like GPT-4 (Achiam et al., 2023)\nand Gemini (Team et al., 2023) offer image-based\napproaches, where we can pass the table as im-\nages to the LLMs. In such cases, visual cues like\ncolor highlighting in tables can influence outcomes.\nSecond, diverse prompting methods for text may\nalso apply to tabular data, which can yield varied\nresults (Wei et al., 2022). Furthermore, the tasks\ninvolving tabular data are diverse, including table\nfact-checking (Chen et al., 2019) and table question\nanswering (Pasupat and Liang, 2015), and table-to-\ntext generation (Novikova et al., 2017), etc.\nIn this paper, we systematically evaluate model\nperformance on tabular data for both textual LLMs\nand multi-modal LLMs. Specifically, we investi-\ngate several research questions, including the ef-\nfectiveness of image-based representation of tab-\nular data and how different text-based or image-\nbased prompt methods affect LLMs’ performance\non table-related tasks. In addition, we provide anal-\nysis and hypothesis of LLMs’ behaviors. Our find-\nings include:\n• LLMs maintain decent performance when we\nuse image-based table representations. Some-\ntimes, image-based table representations can\nmake LLMs perform better.\n• There are nuances in the prompting design for\ntable-related tasks, revealed by our comparisons\nof various prompting methods for text- and\nimage-based table representations.\nTo the best of our knowledge, we are the first to\nstudy how LLMs perform with image-based table\nrepresentations. We believe this paper draws new\ninsights into optimizing table-based information\nprocessing.\nText-Based\nImage-Based\nTask Specific Prompt\nOutput\nTableQA\nTableQA\nVanilla-T\nRow-Identifier\nBracket\nColumn-JSON\nRow-JSON",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8"
    ]
  },
  {
    "question": "How does the newly introduced benchmark, TabIS, differ from previous evaluation methods for table information extraction?, ",
    "answer": "The context provided does not specifically mention a newly introduced benchmark called TabIS or how it differs from previous evaluation methods for table information extraction. Therefore, I cannot answer how TabIS differs as this information is not included in the provided context. If further details about TabIS were available, I could provide a comparison. If you have more specific information or another question, please let me know!",
    "references": [
      "53.0 54.1 50.0\n46.0 46.3\n47.0 52.1\n41.3\n58.0\n65.0\n36.0 44.5\n45.5\n44.6 43.5\n38.0\n38.3 43.4\n21.1\nGMNpro(T)\nGMNpro(V)\nGPT-4 (T)\nGPT-4 (V)\nFinQA LogicNLG TabFact WikiTQ E2E Totto\nFigure 9: Performance comparison between passing the text versus image representations of tables to GPT-4 and\nGeminiPro across FinQA, LogicNLG, TabFact, and WikiTQ by accuracy, and E2E and ToTTo by ROUGE-L scores.\nWe use the linearized table (Vanilla-T) as the text-based representation, the original table image (Vanilla-V) as the\nimage-based representation, and CoT prompting.\n80\n60\n40\n20\n0\n64.0\n56.0\n72.0 77.0\n58.2 63.0\n29.0 33.0 52.0 54.1\n53.0\n30.1\n31.7\n45.3\n44.9 42.8\n42.4\n45.0\n42.3\n39.2\n35.9\n35.6\n35.0 28.0\nGMNpro(T)\nGMNpro(V)\nGPT-4 (T)\nGPT-4 (V)\nFinQA LogicNLG TabFact WikiTQ E2E Totto\nFigure 10: Performance comparison between passing the text versus image representations of tables to GPT-4 and\nGeminiPro across FinQA, LogicNLG, TabFact, and WikiTQ by accuracy, and E2E and ToTTo by ROUGE-L scores.\nWe use the linearized table (Vanilla-T) as the text-based representation, the original table image (Vanilla-V) as the\nimage-based representation, and expert prompting.\nGPT-4 manages to reconstruct a 4-row×10-column\ntable with over 100 words (Table 16).\nBut GPT-4 is more prone to hallucination or\nmessing up with the spatial relations when the table\ngets more complicated. For instance, for Table 17,\nGPT-4 reconstructs it as Table 18. As expected,\nGPT-4 fails to answer the corresponding question\nto this table when using a table image as the in-\nput. However, we notice that when using the text\nrepresentation of the table, GPT-4 also fails to an-\nswer this question. This aligns with what we have\nreported in.\nAs expected, GPT-4 fails to answer the corre-\nsponding question to this table when using a table\nimage as the input. However, we notice that when\nusing the text representation of the table, GPT-4\nalso fails to answer this question. This aligns with\nwhat we have reported in Figure 3 that On FinQA,\nGPT-4 better leverages the image representations\nthan the text representations in general.\nC.5 RQ6: Do multimodal LLMs benefit from\nhaving both formats simultaneously as\ninput?\nTest: the six datasets.\nWe test GPT-4 with vanilla prompting for\nTL;DR Answer: Not generally true.\nFull Answer: As shown in Table 19, it is not\ngenerally true that multimodal LLMs benefit from\nhaving both formats simultaneously as input.\nThere are cases where passing both image and\ntext representations would increase the perfor-\nmance (e.g. GPT-4 on WikiTQ and FinQA). In\nother cases, the performance is comparable to ei-\nther passing tables as text or image representation\nor lies in between.\nD Comparison of Llama Models and\nGPT-4 Models\nTable 20 provides the details of what combina-\ntion of table representation and prompting method\nyields the best performance with respect to the\nLlama-70B and GPT-4 models.\nE Prompt Examples\nFigure 11 gives an example of how we construct\nour prompt for an instance in WikiTQ.\nF LLMs’ Generation Examples on Each\nDataset\nFigure 12 gives examples for WikiTQA, TabFact,\nLogicNLG, and FinQA datasets we use, how many\ncombinations of LLMs, table representations, and\nprompting techniques can answer the question cor-\nDistrict Location Communities served\nAgape Christian Academy Burton Township, Ohio and Troy Township, Ohio Accepts applications prior to the start of each school year\n…\nQuestion: where is saint anslem school located?\nDistrict Location Communities served",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8"
    ]
  },
  {
    "question": "What were the performance results of various LLMs tested using the TabIS benchmark, particularly regarding their understanding of table structures?\n",
    "answer": "The performance results of various Large Language Models (LLMs) using the TabIS benchmark for understanding table structures reveal that these models maintain decent performance, with notable improvements when specific prompting strategies are employed. For example, GPT-4 achieved an accuracy of 52.5% with vanilla prompting and a linearized table representation. Using chain-of-thought prompting, the accuracy improved to 60.3%, demonstrating strong table understanding abilities. It's observed that employing enhancements such as brackets in table representations could sometimes dilute the models’ attention to the original table content, leading to reduced performance [1]. Additionally, when using image representations with colored row distinctions under chain-of-thought prompting, GPT-4's average accuracy improved from 51.8% with vanilla prompting to 60.2%, indicating that effective visual cues can enhance comprehension of table structures [1].",
    "references": [
      "arXiv:2402.12424v5 [cs.LG] 17 Oct 2024\nTables as Texts or Images:\nEvaluating the Table Reasoning Ability of LLMs and MLLMs\nNaihao Deng* , Zhenjie Sun* , Ruiqi He , Aman Sikka ,\nYulong Chen , Lin Ma , Yue Zhang , Rada Mihalcea\nUniversity of Michigan University of Cambridge Westlake University\n{dnaihao, zjsun}@umich.edu\nAbstract\nIn this paper, we investigate the effectiveness\nof various LLMs in interpreting tabular data\nthrough different prompting strategies and data\nformats. Our analyses extend across six bench-\nmarks for table-related tasks such as question-\nanswering and fact-checking. We introduce\nfor the first time the assessment of LLMs’\nperformance on image-based table represen-\ntations. Specifically, we compare five text-\nbased and three image-based table represen-\ntations, demonstrating the role of representa-\ntion and prompting on LLM performance. Our\nstudy provides insights into the effective use\nof LLMs on table-related tasks. Our data is\navailable at: https://github.com/dnaihao/\nTables-as-Texts-or-Images.\n1 Introduction\nRecent years have witnessed an explosion of Large\nLanguage Models (LLMs), with impressive per-\nformance on various Natural Language Processing\n(NLP) tasks (Brown et al., 2020; Touvron et al.,\n2023; Team et al., 2023). Research to date has\nexamined the performance of LLMs for various\naspects and abilities (Bang et al., 2023b; Bubeck\net al., 2023; Akter et al., 2023), but their effec-\ntiveness on structured data such as tables is less\nexplored.\nUnlike unstructured text, tables are systemat-\nically organized structures of a large amount of\ninformation. This characteristic makes tabular\ndata serve as the foundations for numerous ap-\nplications, including medical diagnostics, virtual\npersonal assistants, customer relationship manage-\nment (Hemphill et al., 1990; Dahl et al., 1994;\nAkhtar et al., 2022; Xie et al., 2022), etc.\nThe evaluation of LLMs on processing tabular\ndata involves many challenges. First, there are\nmany ways to represent the information in tables.\n*Contributed equally to this work. Appendix A lists the\ndetailed contributions.\nIf we represent the table in pure text, we may use\nnaive linearization or insert brackets to better rep-\nresent table structures. Meanwhile, emerging mul-\ntimodal LLMs like GPT-4 (Achiam et al., 2023)\nand Gemini (Team et al., 2023) offer image-based\napproaches, where we can pass the table as im-\nages to the LLMs. In such cases, visual cues like\ncolor highlighting in tables can influence outcomes.\nSecond, diverse prompting methods for text may\nalso apply to tabular data, which can yield varied\nresults (Wei et al., 2022). Furthermore, the tasks\ninvolving tabular data are diverse, including table\nfact-checking (Chen et al., 2019) and table question\nanswering (Pasupat and Liang, 2015), and table-to-\ntext generation (Novikova et al., 2017), etc.\nIn this paper, we systematically evaluate model\nperformance on tabular data for both textual LLMs\nand multi-modal LLMs. Specifically, we investi-\ngate several research questions, including the ef-\nfectiveness of image-based representation of tab-\nular data and how different text-based or image-\nbased prompt methods affect LLMs’ performance\non table-related tasks. In addition, we provide anal-\nysis and hypothesis of LLMs’ behaviors. Our find-\nings include:\n• LLMs maintain decent performance when we\nuse image-based table representations. Some-\ntimes, image-based table representations can\nmake LLMs perform better.\n• There are nuances in the prompting design for\ntable-related tasks, revealed by our comparisons\nof various prompting methods for text- and\nimage-based table representations.\nTo the best of our knowledge, we are the first to\nstudy how LLMs perform with image-based table\nrepresentations. We believe this paper draws new\ninsights into optimizing table-based information\nprocessing.\nText-Based\nImage-Based\nTask Specific Prompt\nOutput\nTableQA\nTableQA\nVanilla-T\nRow-Identifier\nBracket\nColumn-JSON\nRow-JSON",
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8",
      "achieve a decent performance (52.5% and 60.3%\nrespectively using the vanilla prompting and lin-\nearized table representation from Table 4), demon-\nstrating their strong table understanding abilities.\nIn such cases, brackets or other kinds of table repre-\nsentations may add extra “workload” to the model,\nwhich dilutes the models’ attention to the original\ntable content and thus leads to worse performance.\nRQ3. How do different image-based prompt\nmethods affect LLMs’ performance on\ntable-related tasks?\nTest: We test the three image-based table rep-\nresentations in Section 3.2 together with vanilla,\nchain-of-thought, and expert prompting. We test\nthe Geminipro and GPT-4 model which can take im-\nages as the input. We average the accuracy scores\nacross FinQA, LogicNLG, TabFact, and WikiTQ.\nAppendix C.3 reports LLMs’ performance on E2E\nand ToTTo datasets.\nTL;DR Answer 3.1: CoT prompting helps\nLLMs reason over images of the table.\nFull Answer 3.1: In Table 5, we observe\nthat chain-of-thought prompting helps multimodal\nLLMs in all image-based table representations. For\ninstance, when using different colors to distinguish\nrows in the table (Row-Color), the average accu-\nracy score for GPT-4 improves from 51.8% by\nvanilla prompting to 60.2% by chain-of-thought\nprompting. By explicitly outlining the reason-\ning process, chain-of-thought prompting may help\nLLMs better understand the context and relation-\nships between different rows and columns in the\ntable, therefore better aligning this visual informa-\ntion with the question text. Such consistent perfor-\nmance improvements suggest that chain-of-thought\nprompting may enhance information fusion across\nthe text and vision modality.\nTL;DR Answer 3.2: Distinguishing rows may\nlead to better performance for LLMs to reason\nover images of the table.\nFull Answer 3.2: In Table 5, under CoT prompt-\ning, GPT4 performs slightly better when using col-\nors to distinguish different rows, which also yields\nthe overall best performance using images of the\ntable. In contrast, under CoT prompting, using\ncolors to distinguish columns yields similar perfor-\nmance to vanilla image (58.0% to 57.8% for GPT-4\nand 45.1% to 46.0% for Geminipro), suggesting\nthat these advanced LLMs may not capture row\ninformation as well as column information.\nFigure 6 shows a WikiTQ example with\nGeminipro model’s predictions. Since the question\nasks about the number of games, it requires the\nmodel to count how many rows satisfy such a con-\ndition. Using colors to distinguish rows may help\nmodels visually segment and categorize the data.\nThis visual differentiation may act as a cognitive\naid, which reduces the complexity of parsing and\ninterpreting the tabular data.\nTL;DR Answer 3.3: The more capable LLM\ndoes not necessarily benefit more from the col-\nored images.\nFull Answer 3.3: In addition, if we use the\nvanilla prompt, the different coloring methods may\neven hurt the performance of GPT-4 (for GPT-4,\ncoloring rows with different colors yields 51.8%\ncompared to 56.2% without adding any color), but\nhelpful for Geminipro (for Geminipro, coloring rows\nwith different colors yields 39.4% compared to\n36.8% without adding any color). This suggests\nthat the effectiveness of how different LLMs can\nleverage colored images varies, and does not de-\npend on the model’s overall performance.\nRep Cues GPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nT N/A T T V N/A V T V V V V+T 34 43 21 10 20 41\n30 51 25 14 16 37\n- 57 30 - 58 34 - 57 28 - 61 38 - - -\n- - -\n- - -\n- - -\nTable 6: Accuracy scores of LLMs on FinQA. We\nuse vanilla prompting across experiments in this table.\nGMNpro represents Geminipro model. We denote text\nand image-based table representations as “T” and “V”\nin the “Rep” column, respectively. The “Cues” column\nindicates how we highlight the relevant cells, where\n“N/A” indicates no information about relevant cells, “T”\nindicates referring to relevant cells in the text, “V” in-"
    ]
  },
  {
    "question": "Who are the authors of the paper?",
    "answer": "The authors of the paper are Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, and Rada Mihalcea [1].",
    "references": [
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2022. PubHealthTab: A public health table-based\ndataset for evidence-based fact checking. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 1–16, Seattle, United States. As-\nsociation for Computational Linguistics.\nSyeda Nahida Akter, Zichun Yu, Aashiq Muhamed,\nTianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera,\nKrish Dholakia, Chenyan Xiong, and Graham Neu-\nbig. 2023. An in-depth look at gemini’s language\nabilities. arXiv preprint arXiv:2312.11444.\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan,\nand Phillip Isola. 2022. Exploring visual prompts\nfor adapting large-scale models. arXiv preprint\narXiv:2203.17274.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023a. A multitask, multilingual,\nmultimodal evaluation of ChatGPT on reasoning, hal-\nlucination, and interactivity. In Proceedings of the\n13th International Joint Conference on Natural Lan-\nguage Processing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 675–718,\nNusa Dua, Bali. Association for Computational Lin-\nguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023b. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nMiao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Zhou\nJingbo, Dejing Dou, and Hui Xiong. 2022. To-\nwards table-to-text generation with pretrained lan-\nguage model: A table structure understanding and\ntext deliberating approach. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 8199–8210, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4)."
    ]
  },
  {
    "question": "What is the title of the paper?",
    "answer": "The title of the paper is \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" [1].",
    "references": [
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2022. PubHealthTab: A public health table-based\ndataset for evidence-based fact checking. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 1–16, Seattle, United States. As-\nsociation for Computational Linguistics.\nSyeda Nahida Akter, Zichun Yu, Aashiq Muhamed,\nTianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera,\nKrish Dholakia, Chenyan Xiong, and Graham Neu-\nbig. 2023. An in-depth look at gemini’s language\nabilities. arXiv preprint arXiv:2312.11444.\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan,\nand Phillip Isola. 2022. Exploring visual prompts\nfor adapting large-scale models. arXiv preprint\narXiv:2203.17274.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023a. A multitask, multilingual,\nmultimodal evaluation of ChatGPT on reasoning, hal-\nlucination, and interactivity. In Proceedings of the\n13th International Joint Conference on Natural Lan-\nguage Processing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 675–718,\nNusa Dua, Bali. Association for Computational Lin-\nguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023b. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nMiao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Zhou\nJingbo, Dejing Dou, and Hui Xiong. 2022. To-\nwards table-to-text generation with pretrained lan-\nguage model: A table structure understanding and\ntext deliberating approach. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 8199–8210, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.",
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–"
    ]
  },
  {
    "question": "What is the abstract of the paper?",
    "answer": "The abstract of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" includes the following points:\n\nThe study investigates the effectiveness of various Large Language Models (LLMs) in interpreting tabular data through different prompting strategies and data formats. Analyses extend across six benchmarks for table-related tasks such as question-answering and fact-checking. For the first time, the evaluation includes the performance of LLMs on image-based table representations, comparing five text-based and three image-based representations. The study provides insights into effective use of LLMs on table-related tasks and highlights the importance of both representation and prompting on model performance. Key findings include that LLMs maintain decent performance with image-based representations, and that nuanced prompting designs significantly impact performance [1].",
    "references": [
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,"
    ]
  },
  {
    "question": "What is the introduction of the paper?",
    "answer": "The introduction of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" discusses the explosion of Large Language Models (LLMs) in recent years, highlighting their impressive performance across various Natural Language Processing (NLP) tasks. However, it notes that less attention has been given to their effectiveness with structured data, such as tables, which are systematically organized and fundamental to many applications, including medical diagnostics and customer relationship management.\n\nThe introduction emphasizes the challenges involved in evaluating LLMs on tabular data, particularly regarding the representation of information within tables and the diverse prompting methods applicable to tabular data. The study systematically evaluates both textual and multimodal LLMs, comparing how different prompting techniques impact performance on tasks involving tables. It establishes the importance of representation and prompting in enhancing model performance and sets the stage for the research questions explored in the study [1].",
    "references": [
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "Wu et al., 2023; Zheng et al., 2024). On the con-\ntrary, to the best of our knowledge, few works have\nprompted these LLMs on tasks involving tabular\ndata.\nFor closed-source LLMs, researchers adopt hard\nprompts to manually craft text prompts with dis-\ncrete tokens (Qiao et al., 2022; Bahng et al., 2022;\nLiu et al., 2023). Wei et al. (2022) develop chain-\nof-though prompting, Xu et al. (2023a) develop\nexpert prompting. In our work, we include the\ncomparison between vanilla, chain-of-thought, and\nexpert prompting for LLMs on table-related tasks.\n3 Experiment Setups\n3.1 Experimented LLMs\nTable 1 describes the LLMs we use for our experi-\nments. We use closed-source models such as GPT-\n3.5 and GPT-4 (Brown et al., 2020; Ouyang et al.,\n2022), and Gemini (Team et al., 2023). We note\nthat GPT-4 and Gemini are multimodal models,\nwhich can take tables as images. For open-source\nmodels, we use the chat models from Llama-2 (Tou-\nvron et al., 2023) families from the 7 billion to the\n70 billion parameter version as they are claimed\nto perform on par with closed-source models like\nChatGPT.*\n*https://huggingface.co/meta-Llama/Llama-2-70b-chat\nMethod Name Table Representation\nVanilla-T c1, c2, · · · , cn, v(1,1), v(1,2), · · · , v(1,n), v(2,1), v(2,2),\n· · · , v(2,n), · · · , v(m,1), v(m,2), · · · , v(m,n).\nRow-Identifier\nc1, c2, · · · , cn, [ROW1] v(1,1), v(1,2), · · · , v(1,n),\n[ROW2] v(2,1), v(2,2), · · · , v(2,n), · · · , [ROWm]\nv(m,1), v(m,2), · · · , v(m,n).\nBracket [ [c1, c2, · · · , cn], [v(1,1), v(1,2), · · · , v(1,n)], [v(2,1),\nv(2,2), · · · , v(2,n)], · · · , [v(m,1), v(m,2), · · · , v(m,n)]].\nColumn-JSON { c1: [v(1,1), v(2,1), · · · , v(m,1)], c2: [v(1,2), v(2,2), · · ·\n, v(m,2)], · · · , c\nn: [v(1,n), v(2,n), · · · , v(m,n)] }.\nRow-JSON\n[{ Row: 1, c1: v(1,1), c2: v(1,2), · · · , c\nn\n: v(1,n)}, { Row: 2,\nc1: v(2,1), c2: v(2,2), · · · , c\nn\n: v(2,n) }, · · · , { Row: m, c1:\nv(m,1), c2: v(m,2), · · · , c\nn\n: v(m,n) }].\nTable 2: Text-based table representation examples. We\nconstruct the examples assuming a table of m rows\nand n columns, where ci denotes the column name of\ncolumn i and v(i,j) denotes the cell value at row i and\ncolumn j. We use colored text to indicate different rows\nin the table to assist readers.\n3.2 Prompting Strategies\nWe explore two ways to represent tables in the\nprompt, Text-Based and Image-Based.",
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,"
    ]
  },
  {
    "question": "What is the methodology of the paper?",
    "answer": "The methodology of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" focuses on systematically evaluating the performance of various Large Language Models (LLMs) and multimodal LLMs on tabular data tasks. The methodology is structured as follows:\n\n1. **Models Tested**: The study employs a range of LLMs, including closed-source models like GPT-3.5, GPT-4, and Gemini, which are capable of processing images, as well as open-source models like Llama-2. The models are chosen based on their reported performance on similar tasks.\n\n2. **Table Representations**: The research explores multiple ways to present tabular data to these models, including:\n   - Text-based representations such as linearized tables, row identifiers, brackets, Column-JSON, and Row-JSON.\n   - Image-based representations where tables are provided as images.\n\n3. **Prompting Strategies**: Different prompting methods are tested to determine their impact on the models' performance. These include:\n   - Vanilla prompting.\n   - Chain-of-thought prompting, which encourages models to articulate their reasoning process explicitly.\n   - Expert prompting, which includes more detailed instructions tailored to specific tasks.\n\n4. **Experimental Setup**: The experiments measure model accuracy across various tasks including question answering, fact checking, and table-to-text generation. Specific benchmarks used for evaluation include FinQA, LogicNLG, TabFact, WikiTQ, E2E, and ToTTo.\n\n5. **Performance Evaluation**: The models' outputs are assessed for accuracy and quality, often using ROUGE-L scores and manual evaluations to ensure that generated responses capture all pertinent information without introducing errors.\n\n6. **Statistical Analysis**: The study employs statistical significance tests to evaluate the differences in performance across different table representations and prompting strategies, enhancing the rigor of the findings [1].",
    "references": [
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8",
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,"
    ]
  },
  {
    "question": "What is the related work of the paper?",
    "answer": "The related work section of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" discusses prior efforts in the field of natural language processing, emphasizing the gap in research pertaining to structured data, particularly tables. Unlike the growing body of work focused on unstructured data, such as images and natural language text, the evaluation of table understanding has been relatively underexplored.\n\nKey studies referenced include works that have experimented with various prompting methods in LLMs, including chain-of-thought prompting and expert prompting, which aid models in reasoning processes. The paper highlights the need for more targeted research in prompting strategies for tasks involving tabular data [1]. Additionally, it acknowledges the contributions of models like GPT and Gemini, which have begun to incorporate table representations into their functionalities but lack comprehensive assessments of their capabilities with both text-based and image-based representations of tables.\n\nThe authors assert that existing literature tends to overlook the performance differences across formats (text vs. image) and various prompting strategies when applied to table-related tasks, presenting an opportunity for further investigation. Overall, the related work underscores the significance of understanding how LLMs can effectively process and reason over structured data formats like tables.",
    "references": [
      "3. For the proportion Z test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, WikiTQ\n(Figure 3).\nC Research Questions Cont’d\nC.1 RQ1 Cont’d. Can we use image-based\nrepresentations of tabular data?\nFigure 9 and Figure 10 show the performance com-\nparison between feeding text representations versus\nimage representations of the table to GPT-4 and\nGeminipro for chain-of-thought and expert prompt-\ning, respectively. The results resemble similar\ntrends as Figure 3.\nC.2 RQ2 Cont’d. How do different text-based\nprompt methods affect LLMs’\nperformance on tabular-related tasks?\nTable 8 reports the averaged accuracy scores across\nFinQA, LogicNLG, TabFact and WikiTQ that use\naccuracy as the metric. Table 9 and Table 11 re-\nport the ROUGE-L scores of LLMs’ generation on\nE2E and ToTTo dataset, respectively. Table 10 re-\nports the scores annotated manually by the authors.\nAs discussed in Section 3.4, the authors manually\ncheck whether the generated sentence captures all\nthe information from the table and does not include\nany additional or misinformation. We assign “1”\nfor sentences who satisfy the criteria and “0” other-\nwise.\nGPT GMNpro\nLlama-2\n3.5 4 7B 13B 70B\nVanilla-T\nV 52.5 E 51.0 CoT 60.3 37.1 28.8 35.3 42.7\n63.8 39.5 29.0 35.1 46.7\n55.2 62.6 53.5 32.1 37.6 48.3\nBracket\nV 50.9 E 47.9 CoT 60.1 38.4 28.4 36.6 42.2\n62.8 39.5 28.1 34.5 45.8\n51.4 61.9 57.3 34.2 39.3 50.0\nColumn-JSON\nV 48.3 E 48.8 CoT 59.5 32.6 24.9 28.8 39.2\n62.8 34.0 26.4 28.2 42.5\n51.2 59.6 53.6 28.0 34.8 42.8\nRow-JSON\nV 49.7 E 53.7 CoT 62.3 41.2 27.9 32.6 40.9\n63.8 39.4 26.4 31.6 45.4\n53.3 62.0 52.1 31.0 35.7 48.4\nRow-Identifier\nV 52.0 E 53.2 CoT 61.2 38.6 27.9 38.5 43.2\n63.0 38.2 26.1 34.0 41.8\n51.6 62.1 56.5 30.6 33.0 45.9\nTable 8: For text-based table representations, averaged\naccuracy scores across FinQA, LogicNLG, TabFact,\nand WikiTQ for different LLMs. “GMNpro” represents\nGeminipro model, “V”, “E”, and “CoT” represent vanilla,\nexpert and chain-of-thought prompting, respectively.\nC.3 RQ3 Cont’d. How do different\nimage-based prompt methods affect\nLLMs’ performance on tabular-related\ntasks?\nTables 12 and 13 report the ROUGE-L scores of\nGPT-4 and Geminipro when we use image repre-\nsentations of tables on E2E and ToTTo dataset,\nrespectively. Table 14 reports the scores annotated\nmanually by the authors.\nC.4 RQ5: Can These LLMs reconstruct\ntables from the image?\nTest: We conduct experiments to explore table re-\nconstruction ideas using GPT4 on E2E and FinQA\ndatasets.\nTL;DR Answer: Mostly yes.\nFull Answer: GPT4 successfully reconstructs\n97/100 tables on the E2E dataset, and 72/100 tables\non the FinQA dataset. We notice that for simpler ta-\nbles, GPT4 can reconstruct tables almost perfectly.\nFor instance, in E2E, the only “mistake” the model\nmakes is to output “é” instead of “e” in “Rainbow\nVegetarian Café”.\nGPT4 also demonstrates a decent capability of\nreconstructing complicated tables. On FinQA,\n80\n60\n40\n20\n0\n56.5\n59.2\n80.0\n71.4\n63.8",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "dicates highlighting relevant cells on the table image,\n“V + T” indicates both highlighting relevant cells on the\ntable image and referring to them in the text.\nRQ4. Does highlighting relevant cells yield a\nbetter performance?\nTest: We test all six LLMs in Section 3.1 on\nFinQA which provides relevant cells in the table\nfor each instance. We refer to the relevant cells\nby adding “Please pay attention to the highlighted\ncells: (row index, column index, cell value)” in the\ntext prompt, or mark them on the table image di-\nrectly. Appendix E provides our prompt examples.\nWe use vanilla prompting in this comparison.\nTL;DR Answer: Yes.\nFull Answer: In Table 6, we notice that in most\ncases, referring LLMs to specific cells helps LLMs\nbetter attend to them, thereby helping LLMs reason\nover the example. However, LLMs’ performance\nmay get hurt when we refer to the relevant cells\nthrough text such as Llama-2-13B and 70B. This\nmay be due to the inherent limitations of textual\ndescriptions for conveying spatial or relational in-\nformation. In order to relate the mentioned cells in\nthe text, the model needs to figure out the connec-\ntion between the mentioned cell and the cell in the\nlinearized table, which can be challenging to the\nmodel given the complicated table structure.\nIn addition, LLMs best attend to the table items\nwhen there are clues from both text and image. In\nTable 6, we observe that marking the relevant cells\non the image while mentioning them through text\nleads to the most correctly answered examples (61\nexamples by GPT-4 and 38 by Geminipro at the last\nrow in Table 6). Such a dual-modality approach\nthat combines visual cues with text references, en-\n∆ Metric\nFinQA 47.0 57.0 +10.0\nLogicNLG 43.4 58.5 +15.1\nTabFact 51.8 74.7 +22.9\nWikiTQ 69.0 86.0 +17.0\nAcc\nE2E 37.1 46.0 + 8.9 ROUGE-L\nToTTo 30.1 47.7 +17.6\nTable 7: Performance scores of the best performed\nopen-source ( ) LLM we test, Llama-2-70B versus\nclosed-source ( ) LLM we test, GPT-4 on different\ndatasets. The closed-source LLMs always outperform\nthe open-source LLMs and we report the performance\ndifference ∆ between them. For consistency across dif-\nferent datasets, we do not include the performances\nwith highlighting cells*in this comparison. Table 20 in\nAppendix D provides what combinations of table repre-\nsentation and prompting method yield this performance.\nDate Country Placing …\n10/31/2008 UK 1st …\n…\nQuestion: What is the number of 1st\nplace finishes across all events?\nGold: 17\nGPT-4: 15 ⨯\nFigure 7: An example from WikiTQ where GPT-4 can-\nnot answer it correctly with bracket table representation\nand chain-of-thought prompting. In addition, for most\nof the table representations and prompting styles, GPT-4\ncannot answer this question correctly.\nhances LLMs’ overall reasoning ability over the\ntabular data.\nRQ5 and RQ6\nWe include two additional research questions and\nour answer to them in Appendix C, including\nwhether these LLMs can reconstruct tables from\nthe image, and whether multimodal LLMs benefit\nfrom having both formats simultaneously as input.\n5 Open Problems to Increase the\nPerformance of LLMs on Tabular Data\nMathematical reasoning. We observe that LLMs\nare not good at arithmetic reasoning similar to the\nfindings in prior works (Hendrycks et al., 2021;\nImani et al., 2023). As shown in Figure 7, sim-\nple arithmetic computing like counting the total\nnumber of rows that satisfy certain conditions (‘1st’\nin Figure 7) still poses challenges even for GPT-4.\n*Except for ToTTo, where the task is to generate the sen-\ntence based on the highlighted cells. On ToTTo, we include\nthe highlight information just in text.\nQuestion: Write a sentence to describe the table.\nName EatType Area\nBlue Spice pub Riverside\nModel Generated\nThe table contains three columns and three rows, with\nL-7B\nL-13B\nthe data “Blue Spice”, “pub”, and “riverside” in the\nfirst column, respectively.\n⨯\nThe table contains information about three different\nrestaurants, including their names, eat types (pub or\nrestaurant), and locations (riverside or not)."
    ]
  },
  {
    "question": "What is the conclusion of the paper?",
    "answer": "The conclusion of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" emphasizes the following key findings:\n\n1. **Effective Representation**: The study demonstrates that both text-based and image-based representations of tables can be effectively utilized by Large Language Models (LLMs). The performance of these models can vary significantly based on the representation used, indicating the importance of selecting the right approach for specific tasks.\n\n2. **Importance of Prompting**: The research highlights how different prompting strategies impact model performance on tasks involving tables. Techniques such as chain-of-thought prompting lead to enhanced reasoning capabilities and better performance in interpreting tabular data.\n\n3. **Multimodal Capabilities**: The findings reveal that multimodal models, which can process both text and image inputs, show improved understanding and reasoning with table-related tasks when provided with both representations simultaneously.\n\n4. **Challenges Remain**: Despite the progress observed, the study also identifies ongoing challenges in arithmetic reasoning and handling complex table structures, underscoring the need for further improvements in LLMs' capabilities for structured data.\n\n5. **Recommendations for Future Work**: The authors suggest avenues for future research, including the development of optimized prompting strategies and representation methods that could further enhance LLMs' abilities in processing complex tabular information [1].",
    "references": [
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–",
      "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2022. PubHealthTab: A public health table-based\ndataset for evidence-based fact checking. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 1–16, Seattle, United States. As-\nsociation for Computational Linguistics.\nSyeda Nahida Akter, Zichun Yu, Aashiq Muhamed,\nTianyue Ou, Alex Bäuerle, Ángel Alexander Cabrera,\nKrish Dholakia, Chenyan Xiong, and Graham Neu-\nbig. 2023. An in-depth look at gemini’s language\nabilities. arXiv preprint arXiv:2312.11444.\nHyojin Bahng, Ali Jahanian, Swami Sankaranarayanan,\nand Phillip Isola. 2022. Exploring visual prompts\nfor adapting large-scale models. arXiv preprint\narXiv:2203.17274.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,\nand Pascale Fung. 2023a. A multitask, multilingual,\nmultimodal evaluation of ChatGPT on reasoning, hal-\nlucination, and interactivity. In Proceedings of the\n13th International Joint Conference on Natural Lan-\nguage Processing and the 3rd Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 675–718,\nNusa Dua, Bali. Association for Computational Lin-\nguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023b. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM confer-\nence on fairness, accountability, and transparency,\npages 610–623.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nMiao Chen, Xinjiang Lu, Tong Xu, Yanyan Li, Zhou\nJingbo, Dejing Dou, and Hui Xiong. 2022. To-\nwards table-to-text generation with pretrained lan-\nguage model: A table structure understanding and\ntext deliberating approach. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 8199–8210, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.",
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4)."
    ]
  },
  {
    "question": "What is the future work of the paper?",
    "answer": "The future work of the paper \"Tables as Texts or Images: Evaluating the Table Reasoning Ability of LLMs and MLLMs\" suggests several avenues for further research, including:\n\n1. **Enhancing Prompting Strategies**: Future efforts could focus on developing and optimizing various prompting techniques that could help improve the performance of LLMs on tasks involving tabular data. This includes exploring more effective methods of combining both textual and visual inputs.\n\n2. **Improving Model Capabilities**: The study indicates a need for advancing the arithmetic reasoning capabilities of LLMs, as current models show limitations in handling numeric tasks accurately, especially in complex scenarios.\n\n3. **Exploration of Multimodal Integrations**: Further research could investigate how multimodal models can best leverage both text and image representations of tables for improved reasoning.\n\n4. **Comprehensive Evaluation Metrics**: Developing better evaluation metrics that can comprehensively assess LLMs' capabilities concerning various types of tabular data would contribute to more meaningful insights into their performance.\n\n5. **Addressing Table Complexity**: Investigating how LLMs can effectively manage and interpret more complex table structures is essential, particularly in real-world applications where data variability is significant [1].",
    "references": [
      "understanding of textual and tabular data. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nLiangyu Zha, Junlin Zhou, Liyao Li, Rui Wang, Qingyi\nHuang, Saisai Yang, Jing Yuan, Changbao Su, Xiang\nLi, Aofeng Su, et al. 2023. Tablegpt: Towards unify-\ning tables, nature language and commands into one\ngpt. arXiv preprint arXiv:2307.08674.\nHongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi\nCao, Fuzheng Zhang, and Zhongyuan Wang. 2020.\nTable fact verification with structure-aware trans-\nformer. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1624–1629, Online. Association for\nComputational Linguistics.\nTianshu Zhang, Xiang Yue, Yifei Li, and Huan Sun.\n2023. Tablellama: Towards open large generalist\nmodels for tables.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and\nYu Su. 2024. Gpt-4v (ision) is a generalist web agent,\nif grounded. arXiv preprint arXiv:2401.01614.\nA Contributions\nIdea Proposal. Naihao Deng proposed the idea\nof evaluating LLMs’ performance on table-related\ntasks with various text-based and image-based table\nrepresentations.\nBackground Survey. Zhenjie Sun conducted a\ncareful survey on table-related tasks. Naihao Deng\ndid the initial background survey on table-related\ntasks when he was a research assistant in Westlake\nUniversity with Yue Zhang.\nImplementation. Zhenjie Sun came up with var-\nious table representations and designed all the\nprompts. Zhenjie Sun also implemented the eval-\nuation pipeline for autonomous metrics. Naihao\nDeng and Zhenjie Sun implemented the pipeline\nfor human evaluation.\nExperiments. Zhenjie Sun and Naihao Deng\nconducted all the experiments. Specifically,\nZhenjie Sun conducted experiments on GPT-3.5,\nGPT-4 with text-based table representations, and\nGeminipro. Naihao Deng conducted experiments\non Llama-2 models and GPT-4 with image-based\ntable representations.\nResult Aggregation. Zhenjie Sun conducted the\nresult aggregation for all the experiments.\nHuman Evaluation. Zhenjie Sun, Naihao Deng,\nRuiqi He, Aman Sikka conducted the human evalu-\nation for the model generation.\nPaper Writing. Naihao Deng drafted the paper.\nZhenjie Sun drafted the prompting strategies and\nmetrics, and actively got involved in discussions of\nresult analysis. Naihao Deng and Zhenjie Sun plot-\nted all the tables and figures. Naihao Deng, Zhenjie\nSun selected examples that appeared in this paper.\nRuiqi He, Aman Sikka provided suggestions on\nexample selections. Rada Mihalcea, Yue Zhang,\nLin Ma, and Yulong Chen participated in discus-\nsions throughout the entire project and provided\nrevisions and feedback on the paper.\nB Statistical Significance Test Results\nWe have observed significant differences between\nLLMs’ performance. We conduct three statistical\nsignificance tests, including Fisher’s Exact test, Mc-\nNemar’s Test, and proportion Z test for the model\npredictions.\nWith p < 0.05:\n1. For Fisher’s Exact test, we find a statistically\nsignificant difference in GPT-4 performance\nbetween T and V inputs for FinQA, Logic-\nNLG, TabFact, WikiTQ (Figure 3), its perfor-\nmance between vanilla and expert prompting\nfor table text input (Table 4), and its perfor-\nmance difference under vanilla and chain-of-\nthought prompting for image-based table rep-\nresentations (Table 5).\n2. For McNemar’s Test, we find statistically sig-\nnificant differences between GPT-4 perfor-\nmance between T and V inputs for FinQA,\nTabFact, and WikiTQ (Figure 3), as well as\nbetween vanilla and expert prompting for ta-\nble text input (Table 4).",
      "42, Toronto, Canada. Association for Computational\nLinguistics.\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto,\nOjasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada\nMihalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to make exceptions: Exploring language\nmodels as accounts of human moral judgment. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 28458–28473. Curran Associates,\nInc.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2021.\nTapex: Table pre-training via learning a neural sql\nexecutor. arXiv preprint arXiv:2107.07653.\nJekaterina Novikova, Ondˇ rej Dušek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\npages 201–206, Saarbrücken, Germany. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. arXiv preprint\narXiv:2212.09597.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748–8763. PMLR.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nAleksandar Shtedritski, Christian Rupprecht, and An-\ndrea Vedaldi. 2023. What does clip know about a red\ncircle? visual prompt engineering for vlms. arXiv\npreprint arXiv:2304.06712.\nGemini Team, Rohan Anil, Sebastian Borgeaud,\nYonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,",
      "Sanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021a. HittER: Hi-\nerarchical transformers for knowledge graph embed-\ndings. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10395–10407, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and\nWilliam Yang Wang. 2020. Logical natural language\ngeneration from open-domain tables. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7929–7942, Online.\nAssociation for Computational Linguistics.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2019. Tabfact: A large-\nscale dataset for table-based fact verification. arXiv\npreprint arXiv:1909.02164.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\nWilliam Yang Wang. 2021b. FinQA: A dataset of nu-\nmerical reasoning over financial data. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3697–3711, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nTianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul\nGroth, and H. V. Jagadish. 2024. Observatory: Char-\nacterizing embeddings of relational tables.\nDeborah A. Dahl, Madeleine Bates, Michael Brown,\nWilliam Fisher, Kate Hunicke-Smith, David Pallett,\nChristine Pao, Alexander Rudnicky, and Elizabeth\nShriberg. 1994. Expanding the scope of the ATIS\ntask: The ATIS-3 corpus. In Human Language Tech-\nnology: Proceedings of a Workshop held at Plains-\nboro, New Jersey, March 8-11, 1994.\nNaihao Deng, Yulong Chen, and Yue Zhang. 2022. Re-\ncent advances in text-to-SQL: A survey of what we\nhave and what we expect. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 2166–2187, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nXiang Deng, Huan Sun, Alyssa Lees, You Wu, and\nCong Yu. 2020. Turl: table understanding through\nrepresentation learning. Proceedings of the VLDB\nEndowment, 14(3):307–319.\nCharles T. Hemphill, John J. Godfrey, and George R.\nDoddington. 1990. The ATIS spoken language sys-\ntems pilot corpus. In Speech and Natural Language:\nProceedings of a Workshop Held at Hidden Valley,\nPennsylvania, June 24-27,1990.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. NeurIPS.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nWonseok Hwang, Jinyeong Yim, Seunghyun Park, and\nMinjoon Seo. 2019. A comprehensive exploration\non wikisql with table-aware word contextualization.\narXiv preprint arXiv:1902.01069.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathPrompter: Mathematical reasoning using large\nlanguage models. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 37–"
    ]
  }
]